## 1.4 并发编程

>date: 2019-01-28

![](../assets/images/14.jpg)

### 1.4.1 多进程

:star: **进程是资源分配的最小单位，线程是`CPU`调度的最小单位。** :star: 

**进程:** **进程是系统进行资源分配和调度的最小单位，是操作系统结构的基础。** 

在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代面向线程设计的计算机结构中，进程是线程的容器。

**多进程:** 同一个时间里，同一个计算机系统中允许两个或两个以上的进程处于**并行**状态，这是多进程。

> **并行：** 同时执行多个任务；
>
> **并发：** 多个任务同时存在，交替执行。
>

**`Python` 中使用多进程：**

```python
import multiprocessing
import time

# 创建函数并将其作为多个进程

def worker_1(interval):
    print("worker_1")
    time.sleep(interval)
    print("end worker_1")

def worker_2(interval):
    print("worker_2")
    time.sleep(interval)
    print("end worker_2")

def worker_3(interval):
    print("worker_3")
    time.sleep(interval)
    print("end worker_3")

if __name__ == "__main__":
    p1 = multiprocessing.Process(target = worker_1, args = (2,))
    p2 = multiprocessing.Process(target = worker_2, args = (3,))
    p3 = multiprocessing.Process(target = worker_3, args = (4,))

    p1.start()
    p2.start()
    p3.start()
    
    # CPU 核心数
    print(("The number of CPU is:" + str(multiprocessing.cpu_count())))

    for p in multiprocessing.active_children():
        print(("child p.name:" + p.name + "  p.id" + str(p.pid)))

    p1.join()
    p2.join()
    p3.join()
    print("END")
```

**结果:**

```python
The number of CPU is:4  #CPU核心数视具体运行环境而定
child p.name:Process-3  p.id1569
child p.name:Process-2  p.id1568
child p.name:Process-1  p.id1567
worker_1
worker_2
worker_3
end worker_1
end worker_2
end worker_3
END
```

### 1.4.2 多线程

**线程:** **线程是 `CPU` 调度和分配的基本单位。** 

线程是建立在进程的基础上的一次程序运行单位。简单的说，进程是线程的容器，线程运行在进程之下，一个进程里面可以有多个线程。

**多线程:** 一个进程中可以有多条执行路径同时执行，一个线程就是进程中的一条执行路径。一般一个进程会有多个(也可是一个)线程。

**`Python` 中使用多线程:**

```python
# 以下代码引用自廖雪峰博客
import time, threading

# 新线程执行的代码:
def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join()
print('thread %s ended.' % threading.current_thread().name)
```

**结果:**

```python
thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.
```

### 1.4.3 协程

**协程:** 又称微线程，纤程，英文名`Coroutine`。协程把应用程序的代码分为多个代码块，正常情况代码自上而下顺序执行。如果代码块`A`运行过程中，能够切换执行代码块`B`，又能够从代码块`B`再切换回去继续执行代码块`A`，这就实现了协程（通常是遇到`IO`操作时切换才有意义）。 

**`Python`对协程的支持是通过`generator`实现的。**

线程的调度是由操作系统负责，协程调度是程序自行负责；与线程相比，协程减少了无谓的操作系统切换。

```python
from collections import deque
 
def sayHello(n):
    while n > 0:
        print("hello~", n)
        yield n
        n -= 1
    print('say hello')
 
def sayHi(n):
    x = 0
    while x < n:
        print('hi~', x)
        yield
        x += 1
    print("say hi")
 
# 使用yield语句，实现简单任务调度器
class TaskScheduler(object):
    def __init__(self):
        self._task_queue = deque()
 
    def new_task(self, task):
        '''
        向调度队列添加新的任务
        '''
        self._task_queue.append(task)
 
    def run(self):
        '''
        不断运行，直到队列中没有任务
        '''
        while self._task_queue:
            task = self._task_queue.popleft()
            try:
                next(task)
                self._task_queue.append(task)
            except StopIteration:
                # 生成器结束
                pass


if __name__ == "__main__":
    sched = TaskScheduler()
    sched.new_task(sayHello(10))
    sched.new_task(sayHi(15))
    sched.run()
```

基于`yield`实现`actor`模型。

```python
from collections import deque
 
class ActorScheduler:
    def __init__(self):
        self._actors = {}
        self._msg_queue = deque()
 
    def new_actor(self, name, actor):
        self._msg_queue.append((actor, None))
        self._actors[name] = actor
 
    def send(self, name, msg):
        actor = self._actors.get(name)
        if actor:
            self._msg_queue.append((actor, msg))
 
    def run(self):
        while self._msg_queue:
            # print("队列：", self._msg_queue)
            actor, msg = self._msg_queue.popleft()
            # print("actor", actor)
            # print("msg", msg)
            try:
                 actor.send(msg)
            except StopIteration:
                 pass
 
 
if __name__ == '__main__':
    def say_hello():
        while True:
            msg = yield
            print("say hello", msg)
 
    def say_hi():
        while True:
            msg = yield
            print("say hi", msg)
 
    def counter(sched):
        while True:
            n = yield
            print("counter:", n)
            if n == 0:
                break
            sched.send('say_hello', n)
            sched.send('say_hi', n)
            sched.send('counter', n-1)
 
    sched = ActorScheduler()
    # 创建初始化 actors
    sched.new_actor('say_hello', say_hello())
    sched.new_actor('say_hi', say_hi())
    sched.new_actor('counter', counter(sched))
 
    sched.send('counter', 10)
    sched.run()
```

### 1.4.4 `GIL`锁

`Python` 提供了`multiprocessing`和`threading`模块可用于多进程和多线程开发。

多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响。

而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。

在 `Python` 中，运行多线程程序，每个线程都会被上一把锁，以防止每个线程因为共享资源，交替运行而乱改数据。这个锁就是**全局解释锁（`GIL`）**。

```python
lock = threading.Lock()

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 进行一些操作:
            change_it(n)
        finally:
            # 操作完了一定要释放锁:
            lock.release()
```

当多个线程同时执行`lock.acquire()`时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。

获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程，这里使用`try...finally`来确保锁一定会被释放。

锁确保某段关键代码只能由一个线程从头到尾完整地执行，实际上是以单线程模式执行，这样程序的效率大大降低了。

由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁。

**死锁：** 指在一组进程中的各个进程均占有不会释放的资源，但因互相申请被其他进程所占用不会释放的资源而处于的一种永久等待状态。

下面是死锁产生的四个必要条件，只要有一个条件不满足，死锁就可以排除：

- 1) 互斥条件：即某个资源在一段时间内只能由一个进程占有，不能同时被两个或两个以上的进程占有。

- 2) 不可抢占条件：进程所获得的资源在未使用完毕之前，资源申请者不能强行地从资源占有者手中夺取资源，而只能由该资源的占有者进程自行释放。

- 3) 占有且申请条件：进程至少已经占有一个资源，但又申请新的资源；由于该资源已被另外进程占有，此时该进程阻塞；但是，它在等待新资源之时，仍继续占用已占有的资源。

- 4) 循环等待条件：存在一个进程等待序列`{P1，P2，...，Pn}`，其中`P1`等待`P2`所占有的某一资源，`P2`等待`P3`所占有的某一源，...，而`Pn`等待`P1`所占有的的某一资源，形成一个进程循环等待环。

**`Python`因为`GIL`的存在，就不管`CPU`中有几个核，单位时间多个核只能跑一个线程，然后时间片轮转。** 

任何`Python`线程执行前，必须先获得`GIL`锁，然后，每执行`100`条字节码，解释器就自动释放`GIL`锁，让别的线程有机会执行。这个`GIL`全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在`Python`中只能交替执行，即使`100`个线程跑在`100`核`CPU`上，也只能用到`1`个核。

* 绕过`GIL`的思路：

1. 绕过 `CPython`，使用 `JPython`（`Java` 实现的 `Python` 解释器）等别的实现；

2. 把关键性能代码，放到别的语言（一般是 `C++`）中实现。

### 1.4.5 计算密集型和`IO`密集型

**计算密集型：** 需要进行大量的计算，消耗`CPU`资源的任务，比如计算圆周率、对视频进行高清解码等等，全靠`CPU`的运算能力。

这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，`CPU`执行任务的效率就越低，所以，要最高效地利用`CPU`，计算密集型任务同时进行的数量应当等于`CPU`的核心数。

计算密集型任务由于主要消耗`CPU`资源，因此，代码运行效率至关重要。`Python`这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用`C`语言编写。

**`IO` 密集型：** 涉及到网络、磁盘`IO`的任务，这类任务的特点是`CPU`消耗很少，任务的大部分时间都在等待`IO`操作完成。

对于`IO`密集型任务，任务越多，`CPU`效率越高，但也有一个限度。常见的大部分任务都是`IO`密集型任务，比如`Web`应用。

`IO`密集型任务执行期间，`99%`的时间都花在`IO`上，花在`CPU`上的时间很少，因此，用运行速度极快的`C`语言替换用`Python`这样运行速度极低的脚本语言，完全无法提升运行效率。对于`IO`密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，`C`语言最差。

### 1.4.6 `IO`模型

> `IO`模型按照主流区分共有以下几种：
>
> * 阻塞`IO（`bloking IO`）
> * 非阻塞`IO`（`non-blocking IO`）
> * 多路复用`IO`（`multiplexing IO`）
> * 信号驱动式`IO`（`signal-driven IO`）
> * 异步`IO`（`asynchronous IO`）

上述的`IO`模型都包含两个阶段：

1. 等待数据准备好（等待数据从网络中到达，数据到达后，被复制到内核的缓冲区中）；

2. 从内核缓冲区向用户缓冲区复制数据。

**基本 `Linux IO` 模型的简单矩阵:**

![IO 矩阵](../assets/images/146_01.png)

* **同步阻塞`IO`**

![同步阻塞IO](../assets/images/146_02.png)

第一个阶段：当用户进程调用了`recv()/recvfrom()`这个系统调用，`kernel`就开始了`IO`的第一个阶段：准备数据（对于网络`IO`来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的`UDP`包。这个时候`kernel`就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。

第二个阶段：当`kernel`一直等到数据准备好了，它就会将数据从`kernel`中拷贝到用户内存，然后`kernel`返回结果，用户进程才解除`block`的状态，重新运行起来。

* **同步非阻塞IO**

![同步非阻塞](../assets/images/146_03.png)

同步非阻塞是指每隔一段时间，询问`kernel`数据是否准备完成，系统完成后，则进程获取数据，继续执行的过程(此过程也称盲等待)。

在网络`IO`时候，非阻塞`IO`也会进行`recvform`系统调用，检查数据是否准备好，与阻塞`IO`不一样，"非阻塞将大的整片时间的阻塞分成`N`多的小的阻塞, 所以进程不断地有机会 '被' `CPU`光顾"。

在非阻塞的`recvform`系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个`error`。进程在返回之后，可以干点别的事情，然后再发起`recvform`系统调用。重复上面的过程，循环往复的进行`recvform`系统调用，这个过程通常被称之为**轮询**。

直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态。

* **`IO`多路复用**

![IO多路复用](../assets/images/146_04.png)

`IO` 多路复用是指同时进行多个系统调用，内核循环查询多个任务的完成状态，只要任一数据到达就进行处理。

其优势在于系统开销下，与多进程和多线程的方式相比，不必创建和维护进程和线程。

`IO`多路复用常用的系统调用有`select`、`poll`、`epoll`函数。

`IO`多路复用的实质是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），`select`， `poll`，`epoll`函数就可以返回，通知程序进行相应的读写操作。

它们的基本原理就是`select`，`poll`，`epoll`会不断的轮询所负责的所有`socket`，当某个`socket`有数据到达了，就通知用户进程。

1) `select`

让一个进程能等待多个文件描述符(`fd`)，而这些`fd`其中任意一个进入到读就绪状态，就返回。

缺点：

- 1) 每次调用`select`都需要把`fd`从用户态拷贝到内核，开销比较大；

- 2) 每次都需要在内核遍历传入的`fd`；

- 3) `select`支持文件数量比较小，默认是`1024`。当然，也可以通过宏定义修改，但会造成效率降低。

2) `poll`

`poll`的实现和`select`非常相似，只是描述`fd`集合的方式不同，`poll`使用`pollfd`结构（链表结构，没有数量限制）而不是`select`的`fd_set`结构，其他的都差不多。

3) `epoll`

使用一个`fd`管理多个`fd`，将用户关系的`fd`的事件存放到内核的一个事件表中，这样在用户空间和内核空间的`copy`只需一次。

`epoll`提供了三个函数，`epoll_create`、`epoll_ctl`和`epoll_wait`。

- 1) `epoll_create`是创建一个`epoll`句柄；

- 2) `epoll_ctl`是注册要监听的事件类型；

- 3) `epoll_wait`则是等待事件的产生。

每次注册新的事件到`epoll`句柄中时（在`epoll_ctl`中指定`EPOLL_CTL_ADD`），会把所有的`fd`拷贝进内核，而不是在`epoll_wait`的时候重复拷贝。`epoll`保证了每个fd在整个过程中只会拷贝一次。

`select`或`poll`每次调用都将当前进程轮流加入`fd`对应的设备等待队列中，而`epoll`只在`epoll_ctl`时会把当前进程挂到设备等待队列，这个操作只进行一次。同时给每个`fd`指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的`fd`加入一个就绪链表。

`epoll_wait`不断轮询就绪链表，对就绪的`fd`执行对应的操作。

**`Apache`采用 `select`，`select` 时间复杂度`O(n)`；`Nginx`采用 `epoll`，`epoll`时间复杂度`O(1)`**

* **信号驱动式`IO`**

信号驱动式`I/O`指允许`socket`进行信号驱动`IO`，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个`SIGIO`信号，可以在信号处理函数中调用`I/O`操作函数处理数据。

![信号驱动式IO](../assets/images/146_05.png)

* **异步非阻塞 `IO`**

相对于同步`IO`，异步`IO`不是顺序执行。用户进程进行`aio_read`系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到`socket`数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。`IO`两个阶段，进程都是非阻塞的。

![异步非阻塞IO](../assets/images/146_06.png)

用户进程发起`aio_read`操作之后，立刻就可以开始去做其它的事。而另一方面，从`kernel`的角度，当它受到一个`asynchronous read`之后，首先它会立刻返回，所以不会对用户进程产生任何`block`。

然后，`kernel`会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，`kernel`会给用户进程发送一个`signal`或执行一个基于线程的回调函数来完成这次 `IO` 处理过程，告诉它`read`操作完成了。

* **`IO`模型总结**

**`blocking`和`non-blocking`区别**

> 调用`blocking IO`会一直`block`住对应的进程直到操作完成，而`non-blocking IO`在`kernel`还准备数据的情况下会立刻返回。
>

**`synchronous IO`和`asynchronous IO`区别**

> 在说明`synchronous IO`和`asynchronous IO`的区别之前，需要先给出两者的定义。`POSIX`的定义是这样子的：
> 
> `A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;An asynchronous I/O operation does not cause the requesting process to be blocked;`
> 
> 两者的区别就在于`synchronous IO`做“`IO operation`”的时候会将`process`阻塞。按照这个定义，之前所述的`blocking IO`，`non-blocking IO`，`IO multiplexing`都属于`synchronous IO`。
> 
> 有人会说，`non-blocking IO`并没有被`block`啊。这里有个非常“狡猾”的地方，定义中所指的“`IO operation`”是指真实的`IO`操作，就是例子中的`recvfrom`这个`system call`。`non-blocking IO`在执行`recvfrom`这个`system call`的时候，如果`kernel`的数据没有准备好，这时候不会`block`进程。但是，当`kernel`中数据准备好的时候，`recvfrom`会将数据从`kernel`拷贝到用户内存中，这个时候进程是被`block`了，在这段时间内，进程是被`block`的。而`asynchronous IO`则不一样，当进程发起`IO` 操作之后，就直接返回再也不理睬了，直到`kernel`发送一个信号，告诉进程说`IO`完成。在这整个过程中，进程完全没有被`block`。


各个`IO Model`的比较如图所示：

![总结](../assets/images/146_07.png)

### 1.4.7 `CPython`


参考链接：

* [廖雪峰的网站](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014319272686365ec7ceaeca33428c914edf8f70cca383000)
* [简书博客](https://www.jianshu.com/p/486b0965c296)
