## 7.2 数据分析方法

>date: 2019-03-19

![](../assets/images/72.jpg)

[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)

### 7.2.1 主成分分析与典型相关分析

#### 主成分分析原理

假设存在二维随机向量$\mathbf{X} = (X_1, X_2)$，并且$E(\mathbf{X}) = 0$，对其进行$n$次观测得到数据$\mathbf{x_i} = (x_{i1}, x_{i2}) (i = 1, 2, ..., n)$

考虑当$X_1$和$X_2$的相关系数的绝对值为$1$，则作为平面上的点，$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$基本分布在某条直线$l$上，若将原坐标系$x_1 O x_2$逆时针旋转一个角度$\theta$得到新的坐标系$y_1 O y_2$，使坐标轴$O y_1$与$l$重合，这时候观测点$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$基本可由它们在$O y_1$上的坐标决定。

$$y_{i1} = x_{i1}\cos \theta + x_{i2} \sin \theta  (i = 1, 2, ..., n)​$$

这时候，$y_{i1}$是原数据的线性组合在$O y_1$轴上的分散性（样本方差）达到最大，即对原变量$(X_1, X_2)$做了线性变换得到新变量$Y_1$。

$$Y_1 = X_1 \cos \theta + X_2 \theta​$$

这时候$Y_1$就能代表之前二维变量$(X_1, X_2)​$的观测值了。

![$Y_1$](../assets/images/721_01.png)

一般情况下，需要将$O x_1$轴逆时针旋转到观测点$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$具有最大分散性的方向上，设转动的角度为$\theta$，则原数据在新坐标系表示为：

$$\begin{cases} y_{i1} = x_{i1}\cos\theta + x_{i2} \sin \theta \\ y_{i2} = -x_{i1}\sin \theta + x_{i2} \cos \theta \end{cases}$$

即：

$$\begin{cases} Y_1 = X_1 \cos \theta + X_2 \sin \theta \\ Y_2 = - X_1 \sin \theta + X_2 \cos \theta \end{cases}​$$

上面的$Y_1, Y_2$代表着原变量$X_1, X_2​$的线性组合，并且$Var(Y_1)​$达到最大，称$Y_1, Y_2​$分别为$(X_1, X_2)​$的第一、第二主成分。

#### 总体主成分

由上，假设$\mathbf{X} = (X_1, X_2, ..., X_p)$为$p$维随机向量，则其协方差矩阵为$Cov(\mathbf{X}) = \Sigma = E[(\mathbf{X} - E(\mathbf{X}))(\mathbf{X} - E(\mathbf{X}))^{T}]$，这是一个$p$阶非负定方阵。

构造$Y_1 = \mathbf{a_{1}}^{T} \mathbf{X} = a_{11} X_1 + a_{12} X_2 + ... + a_{1p} X_p$

需要在约束条件$\mathbf{a_{1}^{T} \mathbf{a_1}} = 1​$下，确定$Var(Y_1) = Var(\mathbf{a_{1}}^{T} \mathbf{X}) = \mathbf{a_{1}^{T}\Sigma \mathbf{a_1}}​$达到最大。

此时得到第一主成分。

而进一步推广可以得到$Y_k = \mathbf{a_{k}}^{T} \mathbf{X} = a_{k1} X_1 + a_{k2} X_2 + ... + a_{kp} X_p$个主成分，

并且每个主成分之间的信息互不重叠，即不互相关，有$Cov(Y_k, Y_i) = \mathbf{a_{k}^{T}\Sigma \mathbf{a_i}} = 0, (i = 1, 2, ..., k - 1)​$

如上所述可以构造$p$个方差大于零的主成分。

* 主成分解法

1) 特征值

$$\mathbf{\Lambda} e_i = \lambda_i e_i$$，$\mathbf{\Lambda}$为矩阵，$e_i$为$\mathbf{\Lambda} $一个的特征向量，$\lambda_i$为$e_i​$的特征值

2) 特征分解矩阵

$\mathbf{\Lambda} $有一组特征向量$e_i$，对这组向量进行正交单位化。

则矩阵$\mathbf{\Lambda} ​$分解为：

$$\mathbf{\Lambda}  = \mathbf{P \Sigma P^{-1}}  =  \mathbf{P \Sigma P^{T}}= Diag(\lambda_1, \lambda_2, ..., \lambda_p)​$$

$$(\mathbf{P \Sigma P^{T}})^{\mathbf{T}} =  \mathbf{P^T \Sigma P}​$$

$\mathbf{P}$为特征向量组成的矩阵，$\mathbf{\Sigma}$为对角阵，对角线元素为特征值

3) 主成分

$\mathbf{X} = (X_1, X_2, ..., X_p)​$的协方差矩阵为$Cov(\mathbf{X}) = \Sigma​$其特征值从大到小排序为$\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_{\gamma} \geq 0​$，对应的正交单位化特征向量为$\mathbf{e_1}, \mathbf{e_2}, ..., \mathbf{e_p}​$

则$\mathbf{X}​$的第$k​$个主成分可表示为：

$$Y_k = \mathbf{e_k^TX} = e_{k1}X_1 + e_{k2}X2 + ... + e_{kp}X_p, k = 1, 2, ..., p​$$

这时候有

$$\begin{cases} Var(Y_k) = \mathbf{e_k^T\Sigma e_k} = \lambda_k \mathbf{e_k^T e_k} = \lambda_k, & k = 1, 2, ..., p \\ Cov(Y_j, Y_k) = \mathbf{e_j^T \Sigma e_k} = \lambda_k \mathbf{e_j^T e_k} = 0, & j \neq k  \end{cases}​$$

这里令$\mathbf{P} = (\mathbf{e_1}, \mathbf{e_2}, ..., \mathbf{e_p})$，其为正交矩阵，假设$\lambda_k$为第$k$个主成分，$Y_k = \mathbf{a_k^TX}$，其中$\mathbf{a_k^T a_k} = 1$，令

$$\mathbf{z_k} = (z_{11}, z_{12}, ..., z_{1p})^T = \mathbf{P^Ta_1}​$$

则$\mathbf{z_k^T z_k} = \mathbf{a_k^T PP^Ta_k} = \mathbf{a_k^T a_k} = 1​$

所以：

$$Var(Y_1) = \mathbf{a_k^T \Sigma a_k} = \mathbf{z_k^T P^T \Sigma P z_k} =  \mathbf{z_k^T} Diag(\lambda_1, \lambda_2, ..., \lambda_p) \mathbf{z_k} = \lambda_1 z_{k1}^2 + \lambda_2 z_{k2}^2 + ... + \lambda_p z_{kp}^2 \leq \lambda_k \mathbf{z_k^T z_k}  = \lambda_k​$$

当$\mathbf{z_k}$为单位基向量，（如$\mathbf{z_1} = (1, 0, ..., 0)$），等号成立，此时$\mathbf{a_k} = \mathbf{Pz_k} = \mathbf{e_k}$。

可以知道，在约束条件$\mathbf{a_{k}^{T} \mathbf{a_k}} = 1$下，$Var(Y_k) = \lambda_k $达到最大，而此时$\mathbf{a_k} = \mathbf{e_k}$

4) 贡献率和累计贡献率

贡献率

$$\frac{\lambda_k}{\sum_{i = 1}^p \lambda_k} = \frac{Var(Y_k)}{\sum_{i=1}^p Var(X_k)}$$

累计贡献率

$$\frac{\sum_{k = 1}^m \lambda_k}{\sum_{i = 1}^p  \lambda_k}$$

* 用途

1) 能降低所研究的数据空间的维数；

2) 通过因子负荷$a_{ki}$的结论，弄清$X_i$变量间的某些关系；

3) 构造回归模型，即把各主成分作为新自变量代替原来自变量做回归分析。

```python
>>> # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
>>> import numpy as np
>>> from sklearn.decomposition import PCA
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> pca = PCA(n_components=2)
>>> pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
>>> print(pca.explained_variance_ratio_) # 累计贡献率
[0.99244289 0.00755711]
```

#### 典型相关分析

随机变量之间$X$和$Y$之间的相关系数

$$\rho_{X,Y} = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}$$

上述式子虽然能了解每对变量$X_i, Y_j$之间的相关性，但是却不能却面反映两组变量间的整体相关性，当两组变量的维数均较大时，只鼓励地了解各对变量$X_i$和$Y_j$之间的相关性，不利于实际问题的全面分析和解决。

因此，借助于主成分分析的思想，可以将两组变量的相关性转化为两个变量的相关性来考虑。即：

$$U = a_1 X_1 + a_2 X_2 + ... + a_p X_p, \\ V = b_1Y_1 + b_2 Y_2 + ... + b_q Y_q$$

其中$\mathbf{a} = (a_1, a_2, ..., a_p), \mathbf{b} = (b_1, b_2, ..., b_q)$，需要$U,V$尽最大的可能提取$\mathbf{X, Y}$的相关性，就称$(U,V)为一对典型相关变量$。

#### 总体的典型变量与典型相关

$$\mathbf{X} = (X_1, X_2, ..., X_p)^T, \mathbf{Y} = (Y_1, Y_2, ... Y_q)^T$$

$(\mathbf{X^T, Y^T})^T = (X_1, X_2, ..., X_p, Y_1, Y_2, ... Y_q)$的协方差矩阵为：

$$\mathbf{\Sigma} = \begin{bmatrix} \mathbf{\Sigma_{11}} & \mathbf{\Sigma_{12}} \\ \mathbf{\Sigma_{21}} & \mathbf{\Sigma_{22}} \end{bmatrix}$$

其中

$$\mathbf{\Sigma_{11}} = Cov(\mathbf{X}) = E[\mathbf{X} - E(\mathbf{X})][\mathbf{X} - E(\mathbf{X})]^T$$

$$\mathbf{\Sigma_{22}} = Cov(\mathbf{Y}) = E[\mathbf{Y} - E(\mathbf{Y})][\mathbf{Y} - E(\mathbf{Y})]^T$$

$$\mathbf{\Sigma_{21}^{T}} = \mathbf{\Sigma_{12}} = Cov(\mathbf{X}, \mathbf{Y}) = E[\mathbf{X} - E(\mathbf{X})][\mathbf{Y} - E(\mathbf{Y})]^T$$

分别考虑$\mathbf{X, Y}$的线性组合

$$U_k = a_{k1} X_1 + a_{k2} X_2 + ... + a_{kp} X_p, \\ V_k = b_{k1}Y_1 + b_{k2} Y_2 + ... + b_{kq} Y_q​$$

其中

$$Var(U_k) = Var(\mathbf{a_k^TX}) = \mathbf{a_k^T \Sigma_{11} a_k} \\ Var(V_k) = Var(\mathbf{b_k^T Y}) = \mathbf{b_k^T \Sigma_{22}b_k} \\ Cov(U_k, V_k) = Cov(\mathbf{a_k^T X, b_k^T Y}) = \mathbf{a_k^T \Sigma_{12} b_k}$$

则$U_k, V_k$的相关系数为：

$$\rho_{U_k, V_k} = \frac{\mathbf{a_k^T \Sigma_{12} b_k}}{\sqrt{\mathbf{a_k^T \Sigma_{11} a_k}} \sqrt{\mathbf{b_k^T \Sigma_{22}b_k}}}​$$

确定$\mathbf{a_k, b_k}​$就能使得$\rho_{U_k, V_k}​$达到最大。

这里为了简化目标函数表达式，增加一组简单的约束，即：

$$\mathbf{a_k^T \Sigma_{11} a_k} = \mathbf{b_k^T \Sigma_{22}b_k} = 1 \\ Cov(U_k, U_j) = Cov(V_k, V_j) = Cov(U_k, V_j) = Cov(V_k, U_j) = 0, 1 \leq j < k$$

此时问题变成了求$\mathbf{a_k^T \Sigma_{12} b_k}$最大值。

* 典型变量与典型相关系数求法

设$\mathbf{X} = (X_1, X_2, ..., X_p)^T, \mathbf{Y} = (Y_1, Y_2, ..., Y_q)^T, \\ Cov(\mathbf{X}) = \Sigma_{11}, Cov(\mathbf{Y}) = \Sigma_{22}, \\ \Sigma_{12} = Cov(\mathbf{X,Y}), \Sigma_{21} = Cov(\mathbf{Y,X}) = \Sigma_{12}^T$

其中$\mathbf{\Sigma_{11}, \Sigma_{22}}$均为满秩矩阵且$p \leq q$

$$A = \mathbf{\Sigma_{11}^{-1}\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}}, B = \mathbf{\Sigma_{22}^{-1} \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12}}$$

设$\rho_1^2 \geq \rho_2^2 ... \geq \rho_p^2$为$p$阶矩阵$A$的特征值，$e_1, e_2, ..., e_p$为相应的正交单位化特征向量。

$f_1, f_2, ..., f_p$为$q$阶矩阵$B$的相应于前$p$个最大特征值的正交单位化特征向量，则$\mathbf{X, Y}$的第$k$对典型相关变量为：

$$U_k = \mathbf{a_k^T X} = \mathbf{e_k^T \Sigma_{11}^{- 1/2} X}, V_k = \mathbf{b_k^T Y} = \mathbf{f_k^T \Sigma_{22}^{- 1/2} Y}, k = 1, 2, ..., p​$$

其相关系数为：

$$\rho_{U_k, V_k} = \rho_k, k = 1, 2, ..., p$$

其中$\mathbf{\Sigma_{11}^{- 1/2}}, \mathbf{\Sigma_{22}^{- 1/2}}$分别为$\mathbf{\Sigma_{11}, \Sigma_{22}}$的平方根矩阵的逆矩阵，$\rho_k$为$\rho_k^2$的正平方根。

* 用途

在实际分析问题中，当面临两组多变量数据，并希望研究两组变量之间的关系时，就要用到典型相关分析。

1) 为了研究扩张性财政政策实施以后对宏观经济发展的影响，就需要考察有关财政政策的一系列指标如财政支出总额的增长率、财政赤字增长率、国债发行额的增长率、税率降低率等与经济发展的一系列指标如国内生产总值增长率、就业增长率、物价上涨率等两组变量之间的相关程度；

2) 为了研究宏观经济走势与股票市场走势之间的关系，就需要考察各种宏观经济指标如经济增长率、失业率、物价指数、进出口增长率等与各种反映股票市场状况的指标如股票价格指数、股票市场融资金额等两组变量之间的相关关系；

3) 在分析评估某种经济投入与产出系统时，研究投入和产出情况之间的联系时，投入情况面可以从人力、物力等多个方面反映，产出情况也可以从产值、利税等方面反映。

```python
>>> # https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA
>>> from sklearn.cross_decomposition import CCA
>>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]
>>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
>>> cca = CCA(n_components=1)
>>> cca.fit(X, Y)
CCA(copy=True, max_iter=500, n_components=1, scale=True, tol=1e-06)
>>> X_c, Y_c = cca.transform(X, Y)
>>> X_c
array([[-1.3373174 ],
       [-1.10847164],
       [ 0.40763151],
       [ 2.03815753]])
>>> Y_c
array([[-0.85511537],
       [-0.70878547],
       [ 0.26065014],
       [ 1.3032507 ]])
>>>
```

### 7.2.2 因子分析

#### 原理

从研究指标相关矩阵内部的依赖关系出发，将一些信息重叠，具有错综复杂关系的变量归结为少数几个不相关的综合因子。

其基本思想是根据相关性大小将变量分组，使得同组内的变量之间相关性较高，不同组的变量不相关或相关性较低，每组变量代表一个基本结构（公共因子）。

#### 数学模型

设$X_i(i = 1, 2, ..., p)$个变量，如果表示为：

$$X_i = \mu_i + a_{i1}F_1 + ... + a_{im}F_m + \varepsilon_i (m \leq p)$$

即:

$$ \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_p \end{bmatrix} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_p \end{bmatrix} + \begin{bmatrix} \alpha_{11} & \alpha_{12} & \cdots & \alpha_{1m} \\ \alpha_{21} & \alpha_{22} & \cdots & \alpha_{2m} \\ \vdots & \vdots & \ddots & \vdots \\ \alpha_{p1} & \alpha_{p2} & \cdots & \alpha_{pm} \end{bmatrix} \begin{bmatrix} F_1 \\ F_2 \\ \vdots \\ F_m \end{bmatrix} + \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_p \end{bmatrix} ​$$

$$ \mathbf{X - \mu} =  \mathbf{AF + \varepsilon}​$$

称$F_1, F_2, ... , F_m$为公共因子，他们的系数$\alpha_{ij}$为因子载荷，$\varepsilon_i$为特殊因子，即满足$cov(F, \varepsilon) = 0, F, \varepsilon $不相关

假设：

$$E(F) = 0, Var(F) = I \\ E(\varepsilon) = 0, Var(\varepsilon) = diag(\sigma_1^2, \sigma_2^2, ..., \sigma_p^2) \\ cov(F, \varepsilon) = 0$$

所以$ \mathbf{X - \mu} =  \mathbf{AF + \varepsilon}​$有

$$Var(\mathbf{X - \mu}) = \mathbf{A}Var(F)\mathbf{A} + Var(\varepsilon) \\ \rightarrow \sum_{\mathbf{x}} = \mathbf{AA'} + Var(\varepsilon) ​$$

其中$Var(\varepsilon) = diag(\sigma_1^2, \sigma_2^2, ..., \sigma_p^2)$

当$diag(\sigma_1^2, \sigma_2^2, ..., \sigma_p^2)​$越小，公共因子共享的成分越多。

#### 统计特征

1) 因子载荷的统计意义

$$X_i = \mu_i + a_{i1}F_1 + ... + a_{im}F_m + \varepsilon_i$$

左右两边同时乘以$F_j$，求数学期望有

$$E(X_iF_j) = a_{i1}E(F_1F_j) + ... + a_{ij}E(F_jF_j) + ... + a_{im} E(F_mF_j) + E(\varepsilon_iF_j)$$

$a_{ij}$的绝对值越大，第$i$个变量与第$j$个公共因子的相关性越高。

2) 公共因子$F_j$方差贡献的统计意义

$$S_j = \sum_{i = 1}^{p}\alpha_{ij}^2$$

称为$F_j(j = 1, 2, ..., m)$对$X_i$的方差贡献和，衡量$F_j$的相对重要性。

#### 主成分估计

设随机变量$\mathbf{x} = (x_1, x_2, ... ,x_p)'$的均值为$\mathbf{\mu}$，协方差为$\sum$，$\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_p \geq 0$为$\sum$的特征值，$\mu_1, \mu_2, ..., \mu_p$为对应的标准化特征向量。

则：

$$\sum = U\begin{bmatrix} \lambda_1 & & & \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_p \end{bmatrix} U' + \mathbf{D} = \mathbf{AA' + D} = [\sqrt{\lambda_1}\mu_1 \sqrt{\lambda_2}\mu_2 ... \sqrt{\lambda_p}\mu_p]\begin{bmatrix} \sqrt{\lambda_1}\mu_1' \\ \sqrt{\lambda_2}\mu_2' \\ \vdots \\ \sqrt{\lambda_p}\mu_p' \end{bmatrix} + \mathbf{D}$$

基于主成分分析的思想，只提取贡献高的前$m$个因子，所以有：

$$\sum \approx [\sqrt{\lambda_1}\mu_1 \sqrt{\lambda_2}\mu_2 ... \sqrt{\lambda_m}\mu_m]\begin{bmatrix} \sqrt{\lambda_1}\mu_1' \\ \sqrt{\lambda_2}\mu_2' \\ \vdots \\ \sqrt{\lambda_m}\mu_m' \end{bmatrix} + \hat D​$$

其中$\hat D = diag(\hat \sigma_1^2, \hat \sigma_2^2, ..., \hat \sigma_p^2)$

$$\hat \sigma_i^2 = s_{ij} - \sum_{j = 1}^{m}\alpha_{ij}^2$$

```python
# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html
>>> from sklearn.datasets import load_digits
>>> from sklearn.decomposition import FactorAnalysis
>>> X, _ = load_digits(return_X_y=True)
>>> X
array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],
       [ 0.,  0.,  0., ..., 10.,  0.,  0.],
       [ 0.,  0.,  0., ..., 16.,  9.,  0.],
       ...,
       [ 0.,  0.,  1., ...,  6.,  0.,  0.],
       [ 0.,  0.,  2., ..., 12.,  0.,  0.],
       [ 0.,  0., 10., ..., 12.,  1.,  0.]])
>>> transformer = FactorAnalysis(n_components=7, random_state=0)
>>> X_transformed = transformer.fit_transform(X)
>>> X_transformed.shape
(1797, 7)
>>> X_transformed
array([[-0.10390271,  0.28198204,  1.93908236, ..., -0.38873227,
        -0.30377059,  0.7951861 ],
       [-0.96261823,  0.18862239, -1.73202491, ..., -0.02628507,
        -0.33583478, -1.4748951 ],
       [-1.07572294, -0.31865454, -1.38943619, ..., -0.99384465,
        -0.52678795,  0.52332889],
       ...,
       [-0.68171725,  0.05397779, -0.80913822, ..., -0.01853757,
        -0.92232969, -0.28710684],
       [-0.34632883, -0.12407529,  1.22216715, ...,  0.21910061,
        -0.29530191, -0.7910442 ],
       [ 0.66672038, -0.92771996, -0.0150591 , ..., -0.84866807,
         0.21509133,  0.51120728]])
```

### 7.2.3 判别分析

#### 两个总体的距离判断

$$\mathbf{x} = (x_1, x_2, ..., x_p)^T, mathbf{y} = (y_1, y_2, ..., y_p)^T$$

为空间上的两点。

设$\mathbf{x, y}$为均值向量为$\mathbf{\mu}$，协方差矩阵为$\mathbf{\sum}$的总体$\mathbf{G}$的两个样本。

则两个样本的马氏平方距离：

$$d^2(\mathbf{x, y}) = (\mathbf{x - y})^T \sum^{-1}(\mathbf{x - y})$$

样本$\mathbf{x}$与总体$\mathbf{G}$的马氏平方距离：

$$d^2(\mathbf{x, G}) = (\mathbf{x - \mu})^T\sum^{-1}(\mathbf{x - \mu})$$

两个总体$\mathbf{G_1, G_2}$之间的马氏平方距离：

$$d^2(\mathbf{G_1, G_2}) = (\mathbf{\mu_1 - \mu_2})^T\sum^{-1}(\mathbf{\mu_1 - \mu_2})$$

* 距离判别规则

$$ \begin{cases} \mathbf{x} \in \mathbf{G_1}, 若d(\mathbf{x, G_1}) \leq d(\mathbf{x, G_2}) \\ \mathbf{x} \in \mathbf{G_2}, 若d(\mathbf{x, G_1}) > d(\mathbf{x, G_2}) \end{cases} $$

当$\mathbf{x}$到$\mathbf{G_1}$的马氏距离不超过到$\mathbf{G_2}$的马氏距离时，判定$\mathbf{x}$来自$\mathbf{G_1}$，反之判定其来自$\mathbf{G_2}$。

假设$\mathbf{G_1} ~ N_p(\mathbf{\mu_1, \sum}), \mathbf{G_2} ~ N_p(\mathbf{\mu_2, \sum})$都为正态总体，概率密度函数分别为：

$$f_1(x) = \frac{1}{(2\pi)^{\frac{p}{2}}|\sum|^{\frac{1}{2}}}\exp\{-\frac{1}{2}(\mathbf{x - \mu_1})^T\sum^{-1}(\mathbf{x - \mu_1})\}$$

$$f_2(x) = \frac{1}{(2\pi)^{\frac{p}{2}}|\sum|^{\frac{1}{2}}}\exp\{-\frac{1}{2}(\mathbf{x - \mu_2})^T\sum^{-1}(\mathbf{x - \mu_2})\}$$

因为两个总体的协方差矩阵相等，所以有以下的判别准则：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 若\frac{f_1(x)}{f_2(x)} \geq 1 \\ \mathbf{x} \in \mathbf{G_2}, 若\frac{f_1(x)}{f_2(x)} < 1 \end{cases}$$

其中$\frac{f_1(x)}{f_2(x)} \geq 1$的充分必要条件为：

$$(\mathbf{x - \mu_1})^T \sum^{-1} (\mathbf{x - \mu_1}) \leq (\mathbf{x - \mu_2})^T \sum^{-1} (\mathbf{x - \mu_2})$$

即：

$$d(\mathbf{x, G_1}) \leq d(\mathbf{x, G_2})$$

1) 当两边协方差相等时

有：

$$d^2(\mathbf{x, G_2}) - d^2(\mathbf{x, G_1}) \\ = (\mathbf{x - \mu_2})^T\sum^{-1}(\mathbf{x - \mu_2}) - (\mathbf{x - \mu_1})^T\sum^{-1}(\mathbf{x - \mu_1}) \\ = -2\mathbf{\mu_2^T\sum^{-1}x} + \mathbf{\mu_2^T\sum^{-1}\mu_2} + 2\mathbf{\mu_1^T\sum^{-1}x} - \mathbf{\mu_1^T\sum^{-1}\mu_1}$$

记：

$$\begin{cases} W_1(\mathbf{x}) = \mathbf{a_1^Tx} + b_1, 其中\mathbf{a_1 = \sum^{-1}\mu_1}, b_1 = -\frac{1}{2}\mathbf{\mu_1^T\sum^{-1}\mu_1} \\ W_2(\mathbf{x}) = \mathbf{a_2^Tx} + b_2, 其中\mathbf{a_2 = \sum^{-1}\mu_2}, b_2 = -\frac{1}{2}\mathbf{\mu_2^T\sum^{-1}\mu_2} \end{cases}$$

即：

$$d^2(\mathbf{x, G_2}) - d^2(\mathbf{x, G_1}) = -2[W_2(x) - W_1(x)]\\ = 2\mathbf{x^T\sum^{-1}(\mu_1 - \mu_2)} + \mathbf{\mu_2^T\sum^{-1}\mu_2 - \mu_1^T\sum^{-1}\mu_1 + \mu_1^T\sum^{-1}\mu_2 - \mu_2^T\sum^{-1}\mu_1} \\ = 2\mathbf{x^T\sum^{-1}(\mu_1 - \mu_2)} - (\mathbf{\mu_1 + \mu_2})^T \sum^{-1}(\mathbf{\mu_1 - \mu_2}) \\ = 2(\mathbf{x - \overline \mu})^T \sum^{-1}(\mathbf{\mu_1 - \mu_2})$$

其中$\mathbf{\overline \mu} = \frac{1}{2}(\mathbf{\mu_1 + \mu_2})​$，记：

$$W(\mathbf{x}) = \mathbf{a^T(x - \overline \mu)}$$

其中$\mathbf{a} = \sum^{-1}(\mathbf{\mu_1 - \mu_2})$

则：

$$d^2(\mathbf{x, G_2}) - d^2(\mathbf{x, G_1}) = 2W(\mathbf{x})$$

距离判别规则化为：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 若 W_1(\mathbf{x}) \geq W_2(\mathbf{x}) \\ \mathbf{x} \in \mathbf{G_2}, 若 W_1(\mathbf{x}) < W_2(\mathbf{x}) \end{cases}$$

由$d^2(\mathbf{x, G_2}) - d^2(\mathbf{x, G_1}) = 2W(\mathbf{x})$可以得到：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 若 W(\mathbf{x}) \geq 0 \\ \mathbf{x} \in \mathbf{G_2}, 若 W(\mathbf{x}) < 0 \end{cases}$$

这里的$W_1(\mathbf{x}), W_2(\mathbf{x}), W(\mathbf{x})$为线性判别函数。

2) 当两边协方差不等时

令：

$$d_1^2(\mathbf{x}) = (\mathbf{x - \mu_1})^T\sum_1^{-1}(\mathbf{x - \mu_1}) \\ d_2^2(\mathbf{x}) = (\mathbf{x - \mu_2})^T\sum_2^{-1}(\mathbf{x - \mu_2})$$

判别准则为：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 若 d_2^2(\mathbf{x}) \geq d_1^2(\mathbf{x}) \\ \mathbf{x} \in \mathbf{G_2}, 若 d_2^2(\mathbf{x}) < d_1^2(\mathbf{x}) \end{cases}$$

其中$d_1^2(\mathbf{x}), d_2^2(\mathbf{x})$分别是样品$\mathbf{x}$到两个总体$\mathbf{G_1, G_2}$的马氏平方距离，皆为$\mathbf{x}$的二次函数，称为二次判别函数。

#### 两个总体的$Bayes$判别

考虑两个$p$维总体$\mathbf{G_1, G_2}$，分别具有概率密度$f_1(x), f_2(x)$，设$\mathbf{G_1, G_2}$出现的先验概率为：

$$p_1 = P(\mathbf{G_1}), p_2 = P(\mathbf{G_2})$$

其中$p_1 + p_2 = 1$

如果$\mathbf{G_1}$的训练样本的容量为$n_1$,$\mathbf{G_2}$的训练样本的容量为$n_2$，而$n = n_1 + n_2$，则先验概率为：

$$p_1 = \frac{n_1}{n}, p_2 = \frac{n_2}{n}$$

对于$p$维指标观测值$\mathbf{x} = (x_1, x_2, ..., x_p)^T$，它取值的空间是$p$维欧式空间$\mathbf{R^p}$，一个判别法实质上是对$\mathbf{R^p}$的一个划分，记为$R_1, R_2$满足条件：

$$R_1 \bigcup R_2 = \mathbf{R^p}, R_1 \bigcap R_2 = \emptyset$$

例如，在两个总体的距离判别中

$$R_1 = \{ \mathbf{x}: d(\mathbf{x, G_1}) \geq d(\mathbf{x, G_2}) \} \\ R_2 = \{ \mathbf{x}: d(\mathbf{x, G_1}) > d(\mathbf{x, G_2}) \}$$

当$\mathbf{x} \in R_1$时，$\mathbf{x}$来自于$\mathbf{G_1}$，反之来自$\mathbf{G_2}$

一个划分$\mathbf{R} = (R_1, R_2)$相当于一个判别准则，在判别准则$\mathbf{R}$下将来自$\mathbf{G_1}$的样品误判为$\mathbf{G_2}$的概率为：

$$P(2 | 1, \mathbf{R}) = \int_{R_2} f_1(x)dx$$

将来自$\mathbf{G_2}$误判为$\mathbf{G_1}$的概率为：

$$P(1 | 2, \mathbf{R}) = \int_{R_1}f_2(x)dx$$

将$\mathbf{G_i}$样品判为$\mathbf{G_j}$的损失为$c(j | i)$。

针对上面的划分，总假定$c(1 | 1) = c(2 | 2) = 0$，$Bayes$判别需要寻求$\mathbf{R} = (R_1, R_2)$使得平均误判损失达到最小。

当得到新样本$\mathbf{x}$后，由$Bayes$公式得到总体$\mathbf{G_1, G_2}$的后验概率为：

$$\begin{cases} P(\mathbf{G_1 | x}) = \frac{p_1f_1(x)}{p_1f_1(x) + p_2f_2(x)} \\ P(\mathbf{G_2 | x}) = \frac{p_2f_2(x)}{p_1f_1(x) + p_2f_2(x)} \end{cases}$$

1) 当$c(2 | 1) = c(1 | 2)$时，两总体$Bayes$判别的一个最优划分为：

$$\begin{cases} R_1 = \{\mathbf{x}: P(\mathbf{G_1 | x}) \geq P(\mathbf{G_2 | x})\} \\ R_2 = \{\mathbf{x}: P(\mathbf{G_1 | x}) < P(\mathbf{G_2 | x})\} \end{cases}$$

所以等损失时两个总体的$Bayes$判别法则为：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 若P(\mathbf{G_1 | x}) \geq P(\mathbf{G_2 | x}) \\ \mathbf{x} \in \mathbf{G_2}, 若P(\mathbf{G_1 | x}) < P(\mathbf{G_2 | x}) \end{cases}$$

这时，最优划分$\mathbf{R}$使得误判概率

$$p^* = p_1P(2 | 1, \mathbf{R}) + p_2P(1 | 2, \mathbf{R})$$

达到最小。

设$c(2 | 1) = c(1 | 2) = c$，则平均误判损失为$cp^*$，因此$\mathbf{R}$使得$cp^*$达到最小等价于使$p^*$达到最下，而：

$$p^* = p_1\int_{R_2}f_1(x)dx + p_2\int_{R_1}f_2(x)dx \\ = \int_{R_1}p_2f_2(x)dx - \int_{R_1}p_1f_1(x)dx \\ + \int_{R_1}p_1f_1(x)dx + \int_{R_2}p_1f_1(x)dx \\ = \int_{R_1}(p_2f_2(x) - p_1f_1(x))dx + p_1$$

其中$\int_{R_1}f_1(x)dx + \int_{R_2}f_1(x)dx = 1$

若取

$$R_1 = \{ \mathbf{x}: p_2f_2(x) \geq p_1f_1(x) \} \\ R_2 = \{ \mathbf{x}: p_2f_2(x) > p_1f_1(x) \} $$

此时可以使得$p^*$达到最小。

根据总体$\mathbf{G_1, G_2}$的后验概率，可以得到等损失时两个总体的$Bayes$判别法则。

2) 当$c(2 | 1) \neq c(1 | 2)$时，因总假定$c(1 | 1) = c(2 | 2) = 0$，对于$\mathbf{G_1, G_2}$而言，它们误判造成的平均损失为：

$$l(1, \mathbf{R}) = c(2 | 1)P(2 | 1, \mathbf{R}) \\ l(2, \mathbf{R}) = c(1 | 2)P(1 | 2, \mathbf{R})$$

因此关于先验概率$p_1, p_2$，误判造成的平均损失为：

$$L = p_1l(1, \mathbf{R}) + p_2l(2, \mathbf{R}) \\ = c(2 | 1)p_1P(2 | 1, \mathbf{R}) + c(1 | 2)p_2P(1 | 2, \mathbf{R})$$

$Bayes$判别使得$L$达到最小的最优划分为：

$$\begin{cases} R_1 = \{ \mathbf{x}: c(2 | 1)P(\mathbf{G_1 | x})\geq c(1 | 2)P(\mathbf{G_2 | x}) \} \\ R_2 = \{ \mathbf{x}: c(2 | 1)P(\mathbf{G_1 | x}) < c(1 | 2)P(\mathbf{G_2 | x}) \}  \end{cases}$$

当$p_1 = p_2$时，有：

$$\begin{cases} R_1 = \{ \mathbf{x}: c(2 | 1)f_1(x) \geq c(1 | 2)f_2(x) \} \\ R_2 = \{ \mathbf{x}: c(2 | 1)f_1(x) < c(1 | 2)f_2(x)  \}  \end{cases}​$$

当$p_1 = p_2, c(2 | 1) = c(1 | 2)$时，有：

$$\begin{cases} R_1 = f_1(x)  \geq f_2(x)  \} \\ R_2 = f_1(x)  < f_2(x)  \}  \end{cases}$$

#### 两正态总体的$Bayes$判别

* $c(2 | 1) = c(1 | 2)$

1) 当$\sum_1 = \sum_2 = \sum$

设总体$\mathbf{G_1, G_2}$的协方差矩阵相等且为$\sum$，其概率密度为

$$f_j(x) = \frac{1}{(z\pi)^{\frac{p}{2}}|\sum|^{\frac{1}{2}}} -exp\{ -\frac{1}{2}(\mathbf{x - \mu_j})^T\sum^{-1}(\mathbf{x - \mu_j}) \}, j = 1, 2$$

则：

$$\ln f_j(x) = -\frac{p}{2}\ln (2\pi) - \frac{1}{2} \ln|\sum| -\frac{1}{2}(\mathbf{x - \mu_j})^T\sum^{-1}(\mathbf{x - \mu_j}), j = 1, 2$$

可以得到：

$$R_1 = \{ \mathbf{x}: p_2f_2(x) \leq p_1f_1(x) \} \\ = \{ \mathbf{x}: \ln f_2(x) + \ln p_2 \leq \ln f_1(x) + \ln p_1\} \\ = \{ \mathbf{x}: -\frac{1}{2}(\mathbf{x - \mu_2})^T\sum^{-1}(\mathbf{x - \mu_2}) + \ln p_2 \\ \leq -\frac{1}{2} (\mathbf{x - \mu_1})^T \sum^{-1}(\mathbf{x - \mu_1}) + \ln p_1 \}$$

记:

$$\begin{cases} W_1(x) = a_1^Tx + b_1,其中 a_1 = \sum^{-1}\mathbf{\mu_1},b_1 = -\frac{1}{2}\mathbf{\mu_1^T \sum^{-1} \mu_1} + \ln p_1 \\  W_2(x) = a_2^Tx + b_2,其中 a_2 = \sum^{-1}\mathbf{\mu_2},b_2 = -\frac{1}{2}\mathbf{\mu_2^T \sum^{-1} \mu_2} + \ln p_2 \end{cases}$$

推导得到：

$$R_1 = \{ \mathbf{x}: W_1(\mathbf{x}) \geq W_2(\mathbf{x}) \} \\ R_2 = \{ \mathbf{x}: W_1(\mathbf{x}) < W_2(\mathbf{x}) \}$$

所以当$\sum_1 = \sum_2 = \sum$时，得到

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 当 W_1(\mathbf{x}) \geq W_2(\mathbf{x}) \\ \mathbf{x} \in \mathbf{G_2}, 当 W_1(\mathbf{x}) < W_2(\mathbf{x}) \end{cases}​$$

当$\mathbf{\mu_1, \mu_2}$及$\sum$未知时，分别由$\mathbf{G_1, G_2}$的训练样本算的均值$\mathbf{\overline x^{(1)}, \overline x^{(2)}}$及协方差矩阵的联合估计$S = \hat \sum$，线性判别函数为：

$$\begin{cases} \hat W_1(x) = \mathbf{\hat a_1^T + \hat b_1}, 其中 \mathbf{\hat a_1} = S^{-1}\mathbf{\overline x^{(1)}}, \hat b_1 = \frac{1}{2}(\mathbf{x^{(1)}})^TS^{-1}\mathbf{\overline x^{(1)}} + \ln p_1 \\ \hat W_2(x) = \mathbf{\hat a_2^T + \hat b_2}, 其中 \mathbf{\hat a_2} = S^{-1}\mathbf{\overline x^{(2)}}, \hat b_1 = \frac{1}{2}(\mathbf{x^{(2)}})^TS^{-1}\mathbf{\overline x^{(2)}} + \ln p_2 \end{cases}​$$

当$p_1 = p_2 = \frac{1}{2}​$时，可在$\hat b_1, \hat b_2​$中略去$\ln p_1, \ln p_2 = \ln \frac{1}{2}​$

进一步考察后验概率$P(G_j | x), j = 1, 2$，因：

$$P(G_j | x) = \frac{p_j f_j(x)}{p_1f_1(x) + p_2f_2(x)}, j= 1, 2$$

又因：

$$p_jf_j(x) = \frac{1}{(2\pi)^{\frac{p}{2}}|\sum|^{\frac{1}{2}}}exp\{ -\frac{1}{2}d_j^2(x) \}, j = 1, 2$$

其中$d_j^2(x) = (\mathbf{x - \mu_j})^T\sum^{-1}(\mathbf{x - \mu_j}) - 2\ln p_j, j =1, 2$称为广义平方距离函数，这时，

$$P(G_j | x) = \frac{exp(-\frac{1}{2}d_j^2(x))}{exp(-\frac{1}{2}d_j^2(x)) + exp(-\frac{1}{2}d_2^2(x))}, j = 1, 2$$

当$\mathbf{\mu_1, \mu_2}$及$\sum$未知时，分别由$\mathbf{\overline x^{(1)}, \overline x^{(2)}}, S$估计得：

$$\hat d_j^2(x) = (\mathbf{x - \overline x^{(j)}})^TS^{-1}(\mathbf{x - \overline x^{(j)}}) - 2 \ln p_j, j = 1, 2$$

后验概率估计为：

$$\hat P(G_j | x) = \frac{exp(-\frac{1}{2}\hat d_j^2(x))}{exp(-\frac{1}{2}\hat d_1^2(x)) + exp(-\frac{1}{2}\hat d_2^2(x))}$$

这时，$Bayes$判别为：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 当 \hat P(\mathbf{G_1 | x}) \geq \hat P(\mathbf{G_2 | x}) \\ \mathbf{x} \in \mathbf{G_2}, 当 \hat P(\mathbf{G_1 | x}) < \hat P(\mathbf{G_2 | x}) \end{cases}$$

2) 当$\sum_1 \neq \sum_2$

设总体$\mathbf{G_1, G_2}$的协方差矩阵分别为$\sum_1, \sum_2$，其概率密度为

$$f_j(x) = \frac{1}{(z\pi)^{\frac{p}{2}}|\sum_j|^{\frac{1}{2}}} -exp\{ -\frac{1}{2}(\mathbf{x - \mu_j})^T\sum_j^{-1}(\mathbf{x - \mu_j}) \}, j = 1, 2$$

则：

$$\ln f_j(x) = -\frac{p}{2}\ln (2\pi) - \frac{1}{2} \ln|\sum_j| -\frac{1}{2}(\mathbf{x - \mu_j})^T\sum_j^{-1}(\mathbf{x - \mu_j}), j = 1, 2$$

所以$L$达到最小的最优划分化为：

$$R_1 = \{ \mathbf{x}: -\frac{1}{2}(mathbf{x - \mu_2})^T\sum_{2}^{-1}(\mathbf{x - \mu_2}) - \frac{1}{2}\ln |\sum_2| + \ln p_2 \\ \leq -\frac{1}{2}(\mathbf{x - \mu_1})^T\sum_1^{-1}(\mathbf{x - \mu_1}) - \frac{1}{2}\ln |\sum_1| + \ln p_1 \}$$

$$R_2 = \{ \mathbf{x}: -\frac{1}{2}(mathbf{x - \mu_2})^T\sum_{2}^{-1}(\mathbf{x - \mu_2}) - \frac{1}{2}\ln |\sum_2| + \ln p_2 \\ > -\frac{1}{2}(\mathbf{x - \mu_1})^T\sum_1^{-1}(\mathbf{x - \mu_1}) - \frac{1}{2}\ln |\sum_1| + \ln p_1 \}$$


记广义平方距离函数为：

$$d_j^2(x) = (\mathbf{x - \mu_j})^T \sum_j^{-1}(\mathbf{x - \mu_j}) + \ln |\sum_j| - 2 \ln p_j, j =1, 2$$

得到最优划分为：

$$\begin{cases} R_1 = \{ \mathbf{x}: d_1^2(\mathbf{x}) \leq d_2^2(\mathbf{x}) \} \\ R_2 = \{ \mathbf{x}: d_1^2(\mathbf{x}) > d_2^2(\mathbf{x}) \} \end{cases}$$

又后验概率

$$P(G_j | x) = \frac{exp(-\frac{1}{2}d_j^2(x))}{exp(-\frac{1}{2}d_j^2(x)) + exp(-\frac{1}{2}d_2^2(x))}, j = 1, 2$$

当$\mathbf{\mu_1, \mu_2}​$及$\sum_1, \sum_2​$未知时，分别由$\mathbf{\overline x^{(1)}, \overline x^{(2)}}, S_1, S_2​$估计得：

$$\hat d_j^2(x) = (\mathbf{x - \overline x^{(j)}})^TS_j^{-1}(\mathbf{x - \overline x^{(j)}}) + \ln|S_j|- 2 \ln p_j, j = 1, 2$$

后验概率估计为：

$$\hat P(G_j | x) = \frac{exp(-\frac{1}{2}\hat d_j^2(x))}{exp(-\frac{1}{2}\hat d_1^2(x)) + exp(-\frac{1}{2}\hat d_2^2(x))}​$$

这时，$Bayes$判别为：

$$\begin{cases} \mathbf{x} \in \mathbf{G_1}, 当 \hat P(\mathbf{G_1 | x}) \geq \hat P(\mathbf{G_2 | x}) \\ \mathbf{x} \in \mathbf{G_2}, 当 \hat P(\mathbf{G_1 | x}) < \hat P(\mathbf{G_2 | x}) \end{cases}​$$

当$p_1 = p_2 = \frac{1}{2}$时，判别函数为：

$$\hat d_j^2(x) = (\mathbf{x - \overline x^{(j)}})^TS_j^{-1}(\mathbf{x - \overline x^{(j)}}) + \ln|S_j|, j = 1, 2$$

跟距离判别中的情况不同，这里训练样本的协方差$S_j$的行列式$|S_j|$视为广义方差，度量了$G_j$的训练样本的分散性。

* $c(2 | 1) \neq c(1 | 2)$

$$R_1 = \{ \mathbf{x}: c(2 | 1)p_1f_1(x) \geq c(1 | 2)p_2f_2(x) \} = \{ \mathbf{x}: d_1^2(x) \leq d_2^2(x) \}$$

其中广义平方距离函数为：

$$d_1^2(x) = (\mathbf{x - \mu_1})^T\sum_1^{-1}(\mathbf{x - \mu_1}) + \ln |\sum_1| - 2\ln(c(2 | 1)p_1) \\ d_2^2(x) = (\mathbf{x - \mu_2})^T\sum_1^{-1}(\mathbf{x - \mu_2}) + \ln |\sum_2| - 2\ln(c(1 | 2)p_2)$$

又：

$$R_2 = \{ \mathbf{x}: d_1^2(x) > d_2^2(x) \}$$

当$\sum_1 = \sum_2 = \sum$时，

$$\begin{cases} R_1 = \{ \mathbf{x}: W_1(\mathbf{x}) \geq W_2(\mathbf{x}) \} \\ R_2 = \{ \mathbf{x}: W_1(\mathbf{x}) < W_2(\mathbf{x}) \} \end{cases}$$

而

$$\begin{cases} W_1(x) = a_1^Tx + b_1,其中 a_1 = \sum^{-1}\mathbf{\mu_1},b_1 = -\frac{1}{2}\mathbf{\mu_1^T \sum^{-1} \mu_1} + \ln(c(2 | 1)p_1) \\  W_2(x) = a_2^Tx + b_2,其中 a_2 = \sum^{-1}\mathbf{\mu_2},b_2 = -\frac{1}{2}\mathbf{\mu_2^T \sum^{-1} \mu_2} + \ln(c(1 | 2)p_2) \end{cases}​$$

```python
# https://scikit-learn.org/stable/modules/lda_qda.html
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)

>>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
>>> qda = QuadraticDiscriminantAnalysis(store_covariances=True)
```

### 7.2.4 聚类分析

#### 样品间相近性的度量

设$\mathbf{X} = (X_1, X_2, ..., X_p)^T$为所关心的$p$个指标，对此指标作$n$次观测得$n$组观测值：

$$\mathbf{x_i} = (x_{i1}, x_{i2}, ..., x_{ip}), i = 1, 2, ..., n$$

称这$n$组观测数据为$n$个样品。

使用距离来衡量这个$p$维空间汇总每个点之间的靠近程度。

不同变量之间计算距离常会受到量纲的影响，可对数据进行标准化，然后在计算距离。

$$x_{ik}^* = \frac{x_{ik} - \overline x_k}{s_k}$$

其中$\overline x_k = \frac{1}{n}\sum_{i = 1}^n x_{ik}, s_k^2 = \frac{1}{n - 1}\sum_{i = 1}^n (x_{ik} - \overline x_k)^2$

常用的距离有：

1) 欧式距离

$$d(\mathbf{x_i, x_j}) = [\sum_{k = 1}^{p} (x_{ik} - x_{jk})^2]^{\frac{1}{2}}$$

2) 绝对距离

$$d(\mathbf{x_i, x_j}) = \sum_{k = 1}^{p} |x_{ik} - x_{jk}|$$

3) $Minkowski$距离

$$d(\mathbf{x_i, x_j}) = [\sum_{k = 1}^p |x_{ik} - x_{jk}|^m]^{\frac{1}{m}}$$

其中$m \geq 1$，又称为$L_m$距离，$L_2$即欧式距离，$L_1$即绝对距离。

4) $Chebyshev$距离

$$d(\mathbf{x_i, x_j}) = max_{1 \leq k \leq p} |x_{ik} - x_{jk}|$$

其是$Minkowski$距离当$m \rightarrow + \infty$时的极限

5) 方差加权距离

$$d(\mathbf{x_i, x_j}) = [\sum_{k = 1}^p \frac{(x_{ik} - x_{jk})^2}{s_k^2}]^{\frac{1}{2}}$$

6) 马氏距离

$$d(\mathbf{x_i, x_j}) = [(\mathbf{x_i - x_j})^T S^{-1}(\mathbf{x_i - x_j})]^{\frac{1}{2}}$$

其中$S$是样品$\mathbf{x_1, x_2, ... x_n}$算得的协方差矩阵：

$$S = \frac{1}{n - 1}\sum_{i = 1}^{a}(\mathbf{x_i - \overline x})(\mathbf{x_i - \overline x})^T$$

其中$\overline x = \frac{1}{n}\sum_{i = 1}^n \mathbf{x_i}$

令$d_{ij} = d(\mathbf{x_i, x_j}), D = (d_{ij})_{n \times n}$形成$n$个样品$\mathbf{x_1, x_2, ... x_n}$两两之间的距离矩阵

$$D = \begin{bmatrix} 0 & d_{12} & \cdots & d_{1n} \\ d_{21} & 0 & \cdots & d_{2n} \\ \vdots & \vdots & & \vdots \\ d_{n1} & d_{n2} & \cdots & 0 \end{bmatrix} $$

其中$d_{ij} = d_{ji}$

#### $K-Means$聚类

* 步骤

1) 创建$k$个点作为$k$个分类的初始聚点；

2) 分别计算剩下的元素到$k$个分类中心的距离，将这些元素分别划归到距离最近的分类中；

3) 根据聚类结果，重新计算$k$个分类的质心，计算方法是取分类中所有元素各自维度的算术平均值；

4) 将样品中全部元素按照新的质心重新聚类；

5) 重复第$4$步，直到聚类结果不再变化，即各个样本与所在分类的质心的均值的误差平方和达到最小：

$$SSE = \sum_{i = 1}{k}\sum_{x \in C_i}||x - \mu_i||_2^2$$

6) 最后，输出聚类结果。

* 优缺点

1) 优点：容易实现；

2) 缺点：可能收敛到局部最小值，在大规模数据上收敛较慢。

适合数据类型：数值型数据

#### 层次聚类

* 步骤

1) 每一个样本点视为一个类；

2) 计算各个类之间的距离，最近的两个类聚合成一个新类；

3) 重复以上过程直至最后只有一类。

* 优缺点

1) 优点：距离和规则的相似度容易定义，限制少；不需要预先制定聚类数；可以发现类的层次关系；可以聚类成其它形状；

2) 缺点：计算复杂度太高；奇异值也能产生很大影响；算法很可能聚类成链状。

```python
# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
>>> from sklearn.cluster import KMeans
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
>>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
>>> kmeans.labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)
>>> kmeans.predict([[0, 0], [12, 3]])
array([1, 0], dtype=int32)
>>> kmeans.cluster_centers_
array([[10.,  2.],
       [ 1.,  2.]])

# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering
>>> from sklearn.cluster import AgglomerativeClustering
>>> import numpy as np
>>> X = np.array([[1, 2], [1, 4], [1, 0],
...               [4, 2], [4, 4], [4, 0]])
>>> clustering = AgglomerativeClustering().fit(X)
>>> clustering 
AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
            connectivity=None, linkage='ward', memory=None, n_clusters=2,
            pooling_func='deprecated')
>>> clustering.labels_
array([1, 1, 1, 0, 0, 0])
```

### 7.2.5 $Bayes$统计模型

设事件$A_1, A_2, .., A_k$构成互补相容的完备事件组，$B$是概率非零的一个事件，则$Bayes$公式是：

$$P(A_i | B) = \frac{P(B|A_i)P(A_i)}{\sum_{j = 1}^k P(B|A_J)P(A_j)}, i = 1, 2, ..., k$$

记$\theta_j$为$A_j$发生时的值，$x_1$为$B$发生的值，$x_2$为$B$不发生的值，$A_1, A_2, ..., A_k$为互不相容完备事件组。

此时：

$$P(\theta = \theta_1 | X = x_1) = \frac{P(X = x_1 | \theta = \theta_1)P(\theta = \theta_1)}{\sum_{j = 1}{k}P(X \ x_1 | \theta = \theta_j)P(\theta = \theta_j)}, i = 1, 2, ..., k$$

即：

$$P(\theta_i | x_1) = \frac{P(x_1 | \theta)\pi(\theta_i)}{\sum_{j = 1}^{k}P(x_1 | \theta_j)\pi(\theta_j)}, i = 1, 2, ..., k$$

条件概率$\{P(\theta_i | x_1), i = 1, 2, ..., k\}$即构成$\theta$的在已知试验结果$X = x_1$的条件下的条件概率分布。

$\pi(\theta_i), i = 1, 2, .., k$代表$\theta$的先验分布，上面的概率分布$P(\theta_i|x_1)$为$\theta$的后验分布。

* $Bayes$统计模型

1) 参数空间$\Theta$上关于参数$\theta$的一个概率分布：$\{\pi(\theta): \theta \in \Theta\}$称为$\theta$的先验分布；

2)样本$\mathbf{x} = (x_1, x_2, ..., x_n)^T$的条件分布族

$$\{f(\mathbf{x} | \theta): \theta \in \Theta\}$$

称为样本分布族；

3) 先验分布$\{ \pi(\theta): \theta \in \Theta \}$与样本分布族$\{ f(\mathbf{x}|\theta): \theta \in \Theta \}$构成$Bayes$统计模型。

