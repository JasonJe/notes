## 7.2 数据分析方法

>date: 2019-03-19

![](../assets/images/72.jpg)

### 7.2.1 主成分分析与典型相关分析

#### 主成分分析原理

假设存在二维随机向量$\mathbf{X} = (X_1, X_2)$，并且$E(\mathbf{X}) = 0$，对其进行$n$次观测得到数据$\mathbf{x_i} = (x_{i1}, x_{i2}) (i = 1, 2, ..., n)$

考虑当$X_1$和$X_2$的相关系数的绝对值为$1$，则作为平面上的点，$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$基本分布在某条直线$l$上，若将原坐标系$x_1 O x_2$逆时针旋转一个角度$\theta$得到新的坐标系$y_1 O y_2$，使坐标轴$O y_1$与$l$重合，这时候观测点$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$基本可由它们在$O y_1$上的坐标决定。

$$y_{i1} = x_{i1}\cos \theta + x_{i2} \sin \theta  (i = 1, 2, ..., n)​$$

这时候，$y_{i1}$是原数据的线性组合在$O y_1$轴上的分散性（样本方差）达到最大，即对原变量$(X_1, X_2)$做了线性变换得到新变量$Y_1$。

$$Y_1 = X_1 \cos \theta + X_2 \theta​$$

这时候$Y_1$就能代表之前二维变量$(X_1, X_2)​$的观测值了。

![$Y_1$](../assets/images/721_01.png)

一般情况下，需要将$O x_1$轴逆时针旋转到观测点$(x_{i1}, x_{i2})(i = 1, 2, ..., n)$具有最大分散性的方向上，设转动的角度为$\theta$，则原数据在新坐标系表示为：

$$\begin{cases} y_{i1} = x_{i1}\cos\theta + x_{i2} \sin \theta \\ y_{i2} = -x_{i1}\sin \theta + x_{i2} \cos \theta \end{cases}$$

即：

$$\begin{cases} Y_1 = X_1 \cos \theta + X_2 \sin \theta \\ Y_2 = - X_1 \sin \theta + X_2 \cos \theta \end{cases}​$$

上面的$Y_1, Y_2$代表着原变量$X_1, X_2​$的线性组合，并且$Var(Y_1)​$达到最大，称$Y_1, Y_2​$分别为$(X_1, X_2)​$的第一、第二主成分。

#### 总体主成分

由上，假设$\mathbf{X} = (X_1, X_2, ..., X_p)$为$p$维随机向量，则其协方差矩阵为$Cov(\mathbf{X}) = \Sigma = E[(\mathbf{X} - E(\mathbf{X}))(\mathbf{X} - E(\mathbf{X}))^{T}]$，这是一个$p$阶非负定方阵。

构造$Y_1 = \mathbf{a_{1}}^{T} \mathbf{X} = a_{11} X_1 + a_{12} X_2 + ... + a_{1p} X_p$

需要在约束条件$\mathbf{a_{1}^{T} \mathbf{a_1}} = 1​$下，确定$Var(Y_1) = Var(\mathbf{a_{1}}^{T} \mathbf{X}) = \mathbf{a_{1}^{T}\Sigma \mathbf{a_1}}​$达到最大。

此时得到第一主成分。

而进一步推广可以得到$Y_k = \mathbf{a_{k}}^{T} \mathbf{X} = a_{k1} X_1 + a_{k2} X_2 + ... + a_{kp} X_p$个主成分，

并且每个主成分之间的信息互不重叠，即不互相关，有$Cov(Y_k, Y_i) = \mathbf{a_{k}^{T}\Sigma \mathbf{a_i}} = 0, (i = 1, 2, ..., k - 1)​$

如上所述可以构造$p$个方差大于零的主成分。

* 主成分解法

1) 特征值

$$\mathbf{\Lambda} e_i = \lambda_i e_i$$，$\mathbf{\Lambda}$为矩阵，$e_i$为$\mathbf{\Lambda} $一个的特征向量，$\lambda_i$为$e_i​$的特征值

2) 特征分解矩阵

$\mathbf{\Lambda} $有一组特征向量$e_i$，对这组向量进行正交单位化。

则矩阵$\mathbf{\Lambda} ​$分解为：

$$\mathbf{\Lambda}  = \mathbf{P \Sigma P^{-1}}  =  \mathbf{P \Sigma P^{T}}= Diag(\lambda_1, \lambda_2, ..., \lambda_p)​$$

$$(\mathbf{P \Sigma P^{T}})^{\mathbf{T}} =  \mathbf{P^T \Sigma P}​$$

$\mathbf{P}$为特征向量组成的矩阵，$\mathbf{\Sigma}$为对角阵，对角线元素为特征值

3) 主成分

$\mathbf{X} = (X_1, X_2, ..., X_p)​$的协方差矩阵为$Cov(\mathbf{X}) = \Sigma​$其特征值从大到小排序为$\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_{\gamma} \geq 0​$，对应的正交单位化特征向量为$\mathbf{e_1}, \mathbf{e_2}, ..., \mathbf{e_p}​$

则$\mathbf{X}​$的第$k​$个主成分可表示为：

$$Y_k = \mathbf{e_k^TX} = e_{k1}X_1 + e_{k2}X2 + ... + e_{kp}X_p, k = 1, 2, ..., p​$$

这时候有

$$\begin{cases} Var(Y_k) = \mathbf{e_k^T\Sigma e_k} = \lambda_k \mathbf{e_k^T e_k} = \lambda_k, & k = 1, 2, ..., p \\ Cov(Y_j, Y_k) = \mathbf{e_j^T \Sigma e_k} = \lambda_k \mathbf{e_j^T e_k} = 0, & j \neq k  \end{cases}​$$

这里令$\mathbf{P} = (\mathbf{e_1}, \mathbf{e_2}, ..., \mathbf{e_p})$，其为正交矩阵，假设$\lambda_k$为第$k$个主成分，$Y_k = \mathbf{a_k^TX}$，其中$\mathbf{a_k^T a_k} = 1$，令

$$\mathbf{z_k} = (z_{11}, z_{12}, ..., z_{1p})^T = \mathbf{P^Ta_1}​$$

则$\mathbf{z_k^T z_k} = \mathbf{a_k^T PP^Ta_k} = \mathbf{a_k^T a_k} = 1​$

所以：

$$Var(Y_1) = \mathbf{a_k^T \Sigma a_k} = \mathbf{z_k^T P^T \Sigma P z_k} =  \mathbf{z_k^T} Diag(\lambda_1, \lambda_2, ..., \lambda_p) \mathbf{z_k} = \lambda_1 z_{k1}^2 + \lambda_2 z_{k2}^2 + ... + \lambda_p z_{kp}^2 \leq \lambda_k \mathbf{z_k^T z_k}  = \lambda_k​$$

当$\mathbf{z_k}$为单位基向量，（如$\mathbf{z_1} = (1, 0, ..., 0)$），等号成立，此时$\mathbf{a_k} = \mathbf{Pz_k} = \mathbf{e_k}$。

可以知道，在约束条件$\mathbf{a_{k}^{T} \mathbf{a_k}} = 1$下，$Var(Y_k) = \lambda_k $达到最大，而此时$\mathbf{a_k} = \mathbf{e_k}$

4) 贡献率和累计贡献率

贡献率

$$\frac{\lambda_k}{\sum_{i = 1}^p \lambda_k} = \frac{Var(Y_k)}{\sum_{i=1}^p Var(X_k)}$$

累计贡献率

$$\frac{\sum_{k = 1}^m \lambda_k}{\sum_{i = 1}^p  \lambda_k}$$

#### 典型相关分析



### 7.2.2 因子分析

### 7.2.3 判别分析

### 7.2.4 聚类分析

### 7.2.5 `Bayes`分析



