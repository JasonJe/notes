## 8.2 监督学习

>date: 2019-06-20

![](../assets/images/82.jpg)

### 8.2.1 `Logistic`回归

将已知存在的数据点利用一条直线进行拟合（该直线被称为最佳拟合直线）的过程称为回归。

`Logistic`回归分类的主要思想就是依据现有已分类的数据样本建立回归公式，以此进行分类。

回归源于最佳拟合，而`Logistic`回归中训练分类器的做法就是利用一些最优算法寻找符合最佳拟合的拟合参数。

#### `Logistic`分布

设$X$是连续的随机变量，且具有如下的分布函数和密度函数：

$$ F(X) = P(X\leq{x}) = \frac{1}{1+\exp^{-\frac{(x-\mu)}{\gamma}}}$$

$$ f(X) = F'(X) = \frac{\exp^{-\frac{x-\mu}{\gamma}}}{\gamma(1+\exp^{-\frac{x-\mu}{\gamma}})^2}$$

其中$\mu$为位置参数，$\gamma > 0$为形状参数。

下图是$F(X),f(X)$的大致图像，人们又把$F(X)$函数称为$sigmoid$函数（以后会经常见到它）。

![Logistic分布](../assets/images/821_01.png)

#### 二项式`Logistic`回归

从`Logistic`分布定义中引申出`Logistic`回归模型。

在上述$X$中，将其分为0、1两类。即随机变量$X$的取值为实数，其分类变量定义为$Y$，每个$x$所属类别可以通过条件概率分布$P(Y|X)$进行表示。
条件概率分布如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x+b)}{1+\exp(\omega \cdot x+b)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x+b)}$$

$x\in R^n$是输入，$Y\in {0, 1}$是输出，$\omega \in R^n$为权值向量，$b \in R$为偏置，$\omega \cdot x$为$\omega$和$x$的内积。

这里将$\omega$和$x$进行扩充，即$\omega = (\omega ^{(1)},\omega ^{(2)},...,\omega ^{(n)},b)$和$x = (x^{(1)},x^{(2)},...,x^{(n)},1)$。

此时的`Logistic`回归模型如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x)}{1+\exp(\omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x)}$$

在考虑一个事件发生为$p$，则不发生的概率为$1-p$，那么这个事件发生的几率（odds）为$\frac{p}{1-p}$。对数几率（log odds）为

$$logit(p) = \log{\frac{p}{1-p}}$$

将上述`Logistic`回归模型代入其中可以得到：

$$\log{\frac{P(Y = 1\mid x)}{1-P(Y = 1\mid x)}} = \omega \cdot x$$

可以看出输出$Y = 1$的对数几率是输入$x$的线性函数，即$Y$可以用$x$的线性函数表示。自然地，在`Logistic`回归模型中，$x$的线性函数值$\omega \cdot x$越接近于$+∞$，其概率越接近于1，反之，$\omega \cdot x$越接近于$-∞$，其概率越接近于0。

#### 参数估计

假设给定训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i \in R^n$，$y_i \in \{0,1\}$，利用极大似然估计法估计模型参数。

设$P(Y=1\mid x) =\pi (x)$，则$P(Y=0\mid x) =1-\pi (x)$

似然函数为：

$$\prod_{i=1}^{N} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$L( \omega ) = \sum\_{i=1}^{N}[y\_i \log \pi(x)+(1-y\_i)\log(1-\pi(x\_i))]  = \sum\_{i=1}^{N}[y\_i\log(\frac{\pi(x\_i)}{1-\pi(x\_i)})+\log(1-\pi(x\_i))] = \sum\_{i=1}^{N}[y\_i( \omega \cdot x\_i)-\log(1+\exp( \omega \cdot x\_i))]$$

此时对$L(\omega)$求最大值，即得到$\omega$的估计值。
为此，后续利用梯度下降法或者拟牛顿法进行该值的求解。

假设$\omega$的极大似然估计值为$\hat \omega$，那么`Logistic`模型为：

$$P(Y=1\mid x) = \frac{\exp(\hat \omega \cdot x)}{1+\exp(\hat \omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\hat \omega \cdot x)}$$

#### 梯度法求解估计值

对上述$L(\omega)$中的$\omega$求导，得到：

$$ \frac{\partial}{\partial \omega}L(\omega) = \sum\_{i=1}^{N} y\_i x\_i - \sum\_{i=1}^{N}\frac{exp(\omega \cdot x\_i)}{1+exp(\omega \cdot x\_i )} x\_i = \sum_{i=1}^{N}(y\_i - \pi(x\_i))x\_i $$

令$\frac{\partial}{\partial \omega}L(\omega) =0$，可以求得其最大的$L(\omega)$对应的$\hat \omega$，但由于无法直接求解，故采用梯度下降法进行求解。

由上求导过程可以知道，$L(\omega)$的梯度为

$$\triangledown\_{\omega}L(\omega) =\sum\_{i=1}^{N}(y\_i - \pi(x\_i)) x\_i$$

故迭代公式（此处为梯度上升算法）为

$$\omega = \omega + \alpha \sum_{i=1}^{N}(y_i - \pi(x_i))x_i$$

其中$\alpha$为步长，当$\mid\mid \triangledown_{\omega}L(\omega) \mid\mid < \varepsilon$，即可求得最大的$\hat \omega$，$\varepsilon$为误差。

#### `Logistic`分类器实现

以下列数据集为例，进行`Logistic`分类器的设计。

|  编号  |  色泽  |  根蒂  |  敲声  |  纹理  |  脐部  |  触感  |  密度   |  含糖率  |  好瓜  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :---: | :--: |
|  1   |  青绿  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.697 | 0.460 |  是   |
|  2   |  乌黑  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  | 0.774 | 0.376 |  是   |
|  3   |  乌黑  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.634 | 0.264 |  是   |
|  4   |  青绿  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  | 0.608 | 0.318 |  是   |
|  5   |  浅白  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.556 | 0.215 |  是   |
|  6   |  青绿  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  | 0.403 | 0.237 |  是   |
|  7   |  乌黑  |  稍蜷  |  浊响  |  稍糊  |  稍凹  |  软粘  | 0.481 | 0.149 |  是   |
|  8   |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  硬滑  | 0.437 | 0.211 |  是   |
|  9   |  乌黑  |  稍蜷  |  沉闷  |  稍糊  |  稍凹  |  硬滑  | 0.666 | 0.091 |  否   |
|  10  |  青绿  |  硬挺  |  清脆  |  清晰  |  平坦  |  软粘  | 0.243 | 0.267 |  否   |
|  11  |  浅白  |  硬挺  |  清脆  |  模糊  |  平坦  |  硬滑  | 0.245 | 0.057 |  否   |
|  12  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  软粘  | 0.343 | 0.099 |  否   |
|  13  |  青绿  |  稍蜷  |  浊响  |  稍糊  |  凹陷  |  硬滑  | 0.639 | 0.161 |  否   |
|  14  |  浅白  |  稍蜷  |  沉闷  |  稍糊  |  凹陷  |  硬滑  | 0.657 | 0.198 |  否   |
|  15  |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  | 0.360 | 0.37  |  否   |
|  16  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  硬滑  | 0.593 | 0.042 |  否   |
|  17  |  青绿  |  蜷缩  |  沉闷  |  稍糊  |  稍凹  |  硬滑  | 0.719 | 0.103 |  否   |

```python
# 导入numpy和pandas库
import numpy as np
import pandas as pd

# 解析数据文件
def loadDataSet(filename):
    dataSet = pd.read_csv(filename, sep = ',', index_col = '编号')

    # 哑变量处理
    featureDict = []
    new_dataSet = pd.DataFrame()
    for i in range(len(dataSet.columns)):
        featureList = dataSet[dataSet.columns[i]]
        classSet = list(set(featureList))
        count = 0
        for feature in classSet:
            d = dict()
            if isinstance(feature, float):# 判断是否为连续变量
                continue
            else:
                featureList[featureList == feature] = count
                d[feature] = count
                count += 1
            featureDict.append(d)
        new_dataSet = pd.concat([new_dataSet, featureList], axis = 1)

    dataMat = [list(new_dataSet.loc[i][:-1]) for i in range(1,len(new_dataSet) + 1)]
    labelMat = list(new_dataSet[new_dataSet.columns[-1]])
    return dataMat, labelMat
```

```python
filename = '821_02.txt'
dataMat, labelMat = loadDataSet(filename)

import pprint
pprint.pprint(dataMat)
pprint.pprint(labelMat)

'''
[[1, 1, 2, 2, 0, 0, 0.69700000000000006, 0.46000000000000002],
[0, 1, 1, 2, 0, 0, 0.77400000000000002, 0.376],
[0, 1, 2, 2, 0, 0, 0.63400000000000001, 0.26400000000000001],
[1, 1, 1, 2, 0, 0, 0.60799999999999998, 0.318],
[2, 1, 2, 2, 0, 0, 0.55600000000000005, 0.215],
[1, 2, 2, 2, 1, 1, 0.40299999999999997, 0.23699999999999999],
[0, 2, 2, 0, 1, 1, 0.48100000000000004, 0.14899999999999999],
[0, 2, 2, 2, 1, 0, 0.43700000000000006, 0.21100000000000002],
[0, 2, 1, 0, 1, 0, 0.66599999999999993, 0.090999999999999998],
[1, 0, 0, 2, 2, 1, 0.24299999999999999, 0.26700000000000002],
[2, 0, 0, 1, 2, 0, 0.245, 0.057000000000000002],
[2, 1, 2, 1, 2, 1, 0.34299999999999997, 0.099000000000000005],
[1, 2, 2, 0, 0, 0, 0.63900000000000001, 0.161],
[2, 2, 1, 0, 0, 0, 0.65700000000000003, 0.19800000000000001],
[0, 2, 2, 2, 1, 1, 0.35999999999999999, 0.37],
[2, 1, 2, 1, 2, 0, 0.59299999999999997, 0.042000000000000003],
[1, 1, 1, 0, 1, 0, 0.71900000000000008, 0.10300000000000001]]
[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
'''
```

可以看到，该操作将字符串型的分类变量解析成为了数值型的分类变量，从而方便了将要进行的数值运算。

* 利用梯度上升法求最佳回归系数

在前面的定义中，`Logistic`分布函数$F(X)$又被称为$sigmoid$函数，其对数几率为

$$\log{\frac{P(Y = 1\mid x)}{1-P(Y = 1\mid x)}} = \omega \cdot x$$

即将$sigmoid$函数的输入可以看作是：

$$z = \omega ^{(1)}x^{(1)}+\omega ^{(2)}x^{(2)}+,...,+\omega ^{(n)}x^{(n)}  = \omega^{T} \cdot x$$

由此，定义下面的代码进行梯度上升法的实现：

```python
# 定义Sigmoid函数
def sigmoid(inX):
    return 1.0/(1 + np.exp(- inX))

# 随机的梯度上升法
def gradAscent(dataMatIn, classLabels, numIter = 150):
    # 获得行数和列数，即样本数和特征数
    m, n = np.shape(dataMatIn)
    # 权重初始化
    weights = np.ones(n)
    for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0 + j + i) + 0.01
            randIndex = int(np.random.uniform(0, len(dataIndex)))
            h = sigmoid(sum(dataMatIn[randIndex] * weights))
            error = classLabels[randIndex] - h
            weights = weights + np.dot(alpha * error, dataMatIn[randIndex])
    return weights

weights = gradAscent(dataMat, labelMat)
print(weights)

'''
[-1.91739131 -2.37320272  3.30085298  1.32020706 -2.30328752  0.58413608
      0.84630395 -0.63702599]
'''
```

* 分类器设计

由上，求出了相应的输入特征对应权重，利用对数几率公式，可以简单实现分类的效果，相关设计代码如下：

```python
def classfy(testdir, weights):
    dataMat, labelMat = loadDataSet(testdir)
    dataMat = np.mat(dataMat)
    weights = np.mat(weights)
    h = sigmoid(dataMat * weights.transpose())
    h = h.tolist()
    m = len(h)
    error = 0.0
    for i in range(m):
        if h[i][0] > 0.5:
            print(int(labelMat[i]),'is classfied as: 1')
            if int(labelMat[i])!=1:
                error += 1
                print('error')
        else:
            print(int(labelMat[i]),'is classfied as: 0')
            if int(labelMat[i])!=0:
                error += 1
                print('error')
    print('error rate is:','%.4f' %(error/m))

print(classfy(filename, weights))
'''
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 1
error
0 is classfied as: 0
0 is classfied as: 1
error
0 is classfied as: 0
0 is classfied as: 0
error rate is: 0.1176
'''
```

将训练的数据样本进行测试，可以看出上述分类中，只有两个样本被分类错误了，准确度达到了$88.24%$，分类效果不错。

* `Scikit-Learn`库简单实现`Logistic`分类

简单通过`Scikit-Learn`库实现`Logistic`的分类：

```python
from sklearn.linear_model import LogisticRegression

X, Y = loadDataSet(filename)

clf = LogisticRegression()
clf.fit(X, Y)
y_pred = clf.predict(X)
accuracy = np.mean(Y == y_pred)
print('准确度为：', accuracy)
'''
准确度为： 0.882352941176
'''
```

可以看出，上述实现的准确度与设计的分类器的准确度基本一样，效果不错。

* `Logistic`回归分类的优缺点：

1. 优点：计算代价低，易于理解和实现；
2. 缺点：易欠拟合，分类精度普遍不高。

### 8.2.2 决策树



### 8.2.3 朴素贝叶斯法

### 8.2.4 `SVM`