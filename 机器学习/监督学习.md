## 8.2 监督学习

>date: 2019-06-20

![](../assets/images/82.jpg)

### 8.2.1 `Logistic`回归

将已知存在的数据点利用一条直线进行拟合（该直线被称为最佳拟合直线）的过程称为回归。

`Logistic`回归分类的主要思想就是依据现有已分类的数据样本建立回归公式，以此进行分类。

回归源于最佳拟合，而`Logistic`回归中训练分类器的做法就是利用一些最优算法寻找符合最佳拟合的拟合参数。

#### `Logistic`分布

设$X$是连续的随机变量，且具有如下的分布函数和密度函数：

$$ F(X) = P(X\leq{x}) = \frac{1}{1+\exp^{-\frac{(x-\mu)}{\gamma}}}$$

$$ f(X) = F'(X) = \frac{\exp^{-\frac{x-\mu}{\gamma}}}{\gamma(1+\exp^{-\frac{x-\mu}{\gamma}})^2}$$

其中$\mu$为位置参数，$\gamma > 0$为形状参数。

下图是$F(X),f(X)$的大致图像，人们又把$F(X)$函数称为$sigmoid$函数（以后会经常见到它）。

![Logistic分布](../assets/images/821_01.png)

#### 二项式`Logistic`回归

从`Logistic`分布定义中引申出`Logistic`回归模型。

在上述$X$中，将其分为0、1两类。即随机变量$X$的取值为实数，其分类变量定义为$Y$，每个$x$所属类别可以通过条件概率分布$P(Y|X)$进行表示。
条件概率分布如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x+b)}{1+\exp(\omega \cdot x+b)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x+b)}$$

$x\in R^n$是输入，$Y\in {0, 1}$是输出，$\omega \in R^n$为权值向量，$b \in R$为偏置，$\omega \cdot x$为$\omega$和$x$的内积。

这里将$\omega$和$x$进行扩充，即$\omega = (\omega ^{(1)},\omega ^{(2)},...,\omega ^{(n)},b)$和$x = (x^{(1)},x^{(2)},...,x^{(n)},1)$。

此时的`Logistic`回归模型如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x)}{1+\exp(\omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x)}$$

在考虑一个事件发生为$p$，则不发生的概率为$1-p$，那么这个事件发生的几率（odds）为$\frac{p}{1-p}$。对数几率（log odds）为

$$logit(p) = \log{\frac{p}{1-p}}$$

将上述`Logistic`回归模型代入其中可以得到：

$$\log{\frac{P(Y = 1\mid x)}{1-P(Y = 1\mid x)}} = \omega \cdot x$$

可以看出输出$Y = 1$的对数几率是输入$x$的线性函数，即$Y$可以用$x$的线性函数表示。自然地，在`Logistic`回归模型中，$x$的线性函数值$\omega \cdot x$越接近于$+∞$，其概率越接近于1，反之，$\omega \cdot x$越接近于$-∞$，其概率越接近于0。

#### 参数估计

假设给定训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i \in R^n$，$y_i \in \{0,1\}$，利用极大似然估计法估计模型参数。

设$P(Y=1\mid x) =\pi (x)$，则$P(Y=0\mid x) =1-\pi (x)$

似然函数为：

$$\prod_{i=1}^{N} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$L( \omega ) = \sum\_{i=1}^{N}[y\_i \log \pi(x)+(1-y\_i)\log(1-\pi(x\_i))]  = \sum\_{i=1}^{N}[y\_i\log(\frac{\pi(x\_i)}{1-\pi(x\_i)})+\log(1-\pi(x\_i))] = \sum\_{i=1}^{N}[y\_i( \omega \cdot x\_i)-\log(1+\exp( \omega \cdot x\_i))]$$

此时对$L(\omega)$求最大值，即得到$\omega$的估计值。
为此，后续利用梯度下降法或者拟牛顿法进行该值的求解。

假设$\omega$的极大似然估计值为$\hat \omega$，那么`Logistic`模型为：

$$P(Y=1\mid x) = \frac{\exp(\hat \omega \cdot x)}{1+\exp(\hat \omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\hat \omega \cdot x)}$$

#### 梯度法求解估计值

对上述$L(\omega)$中的$\omega$求导，得到：

$$ \frac{\partial}{\partial \omega}L(\omega) = \sum\_{i=1}^{N} y\_i x\_i - \sum\_{i=1}^{N}\frac{exp(\omega \cdot x\_i)}{1+exp(\omega \cdot x\_i )} x\_i = \sum_{i=1}^{N}(y\_i - \pi(x\_i))x\_i $$

令$\frac{\partial}{\partial \omega}L(\omega) =0$，可以求得其最大的$L(\omega)$对应的$\hat \omega$，但由于无法直接求解，故采用梯度下降法进行求解。

由上求导过程可以知道，$L(\omega)$的梯度为

$$\triangledown\_{\omega}L(\omega) =\sum\_{i=1}^{N}(y\_i - \pi(x\_i)) x\_i$$

故迭代公式（此处为梯度上升算法）为

$$\omega = \omega + \alpha \sum_{i=1}^{N}(y_i - \pi(x_i))x_i$$

其中$\alpha$为步长，当$\mid\mid \triangledown_{\omega}L(\omega) \mid\mid < \varepsilon$，即可求得最大的$\hat \omega$，$\varepsilon$为误差。

#### `Logistic`分类器实现

以下列数据集为例，进行`Logistic`分类器的设计。

|  编号  |  色泽  |  根蒂  |  敲声  |  纹理  |  脐部  |  触感  |  密度   |  含糖率  |  好瓜  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :---: | :--: |
|  1   |  青绿  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.697 | 0.460 |  是   |
|  2   |  乌黑  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  | 0.774 | 0.376 |  是   |
|  3   |  乌黑  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.634 | 0.264 |  是   |
|  4   |  青绿  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  | 0.608 | 0.318 |  是   |
|  5   |  浅白  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  | 0.556 | 0.215 |  是   |
|  6   |  青绿  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  | 0.403 | 0.237 |  是   |
|  7   |  乌黑  |  稍蜷  |  浊响  |  稍糊  |  稍凹  |  软粘  | 0.481 | 0.149 |  是   |
|  8   |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  硬滑  | 0.437 | 0.211 |  是   |
|  9   |  乌黑  |  稍蜷  |  沉闷  |  稍糊  |  稍凹  |  硬滑  | 0.666 | 0.091 |  否   |
|  10  |  青绿  |  硬挺  |  清脆  |  清晰  |  平坦  |  软粘  | 0.243 | 0.267 |  否   |
|  11  |  浅白  |  硬挺  |  清脆  |  模糊  |  平坦  |  硬滑  | 0.245 | 0.057 |  否   |
|  12  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  软粘  | 0.343 | 0.099 |  否   |
|  13  |  青绿  |  稍蜷  |  浊响  |  稍糊  |  凹陷  |  硬滑  | 0.639 | 0.161 |  否   |
|  14  |  浅白  |  稍蜷  |  沉闷  |  稍糊  |  凹陷  |  硬滑  | 0.657 | 0.198 |  否   |
|  15  |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  | 0.360 | 0.37  |  否   |
|  16  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  硬滑  | 0.593 | 0.042 |  否   |
|  17  |  青绿  |  蜷缩  |  沉闷  |  稍糊  |  稍凹  |  硬滑  | 0.719 | 0.103 |  否   |

```python
# 导入numpy和pandas库
import numpy as np
import pandas as pd

# 解析数据文件
def loadDataSet(filename):
    dataSet = pd.read_csv(filename, sep = ',', index_col = '编号')

    # 哑变量处理
    featureDict = []
    new_dataSet = pd.DataFrame()
    for i in range(len(dataSet.columns)):
        featureList = dataSet[dataSet.columns[i]]
        classSet = list(set(featureList))
        count = 0
        for feature in classSet:
            d = dict()
            if isinstance(feature, float):# 判断是否为连续变量
                continue
            else:
                featureList[featureList == feature] = count
                d[feature] = count
                count += 1
            featureDict.append(d)
        new_dataSet = pd.concat([new_dataSet, featureList], axis = 1)

    dataMat = [list(new_dataSet.loc[i][:-1]) for i in range(1,len(new_dataSet) + 1)]
    labelMat = list(new_dataSet[new_dataSet.columns[-1]])
    return dataMat, labelMat
```

```python
filename = '821_02.txt'
dataMat, labelMat = loadDataSet(filename)

import pprint
pprint.pprint(dataMat)
pprint.pprint(labelMat)

'''
[[1, 1, 2, 2, 0, 0, 0.69700000000000006, 0.46000000000000002],
[0, 1, 1, 2, 0, 0, 0.77400000000000002, 0.376],
[0, 1, 2, 2, 0, 0, 0.63400000000000001, 0.26400000000000001],
[1, 1, 1, 2, 0, 0, 0.60799999999999998, 0.318],
[2, 1, 2, 2, 0, 0, 0.55600000000000005, 0.215],
[1, 2, 2, 2, 1, 1, 0.40299999999999997, 0.23699999999999999],
[0, 2, 2, 0, 1, 1, 0.48100000000000004, 0.14899999999999999],
[0, 2, 2, 2, 1, 0, 0.43700000000000006, 0.21100000000000002],
[0, 2, 1, 0, 1, 0, 0.66599999999999993, 0.090999999999999998],
[1, 0, 0, 2, 2, 1, 0.24299999999999999, 0.26700000000000002],
[2, 0, 0, 1, 2, 0, 0.245, 0.057000000000000002],
[2, 1, 2, 1, 2, 1, 0.34299999999999997, 0.099000000000000005],
[1, 2, 2, 0, 0, 0, 0.63900000000000001, 0.161],
[2, 2, 1, 0, 0, 0, 0.65700000000000003, 0.19800000000000001],
[0, 2, 2, 2, 1, 1, 0.35999999999999999, 0.37],
[2, 1, 2, 1, 2, 0, 0.59299999999999997, 0.042000000000000003],
[1, 1, 1, 0, 1, 0, 0.71900000000000008, 0.10300000000000001]]
[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
'''
```

可以看到，该操作将字符串型的分类变量解析成为了数值型的分类变量，从而方便了将要进行的数值运算。

* 利用梯度上升法求最佳回归系数

在前面的定义中，`Logistic`分布函数$F(X)$又被称为$sigmoid$函数，其对数几率为

$$\log{\frac{P(Y = 1\mid x)}{1-P(Y = 1\mid x)}} = \omega \cdot x$$

即将$sigmoid$函数的输入可以看作是：

$$z = \omega ^{(1)}x^{(1)}+\omega ^{(2)}x^{(2)}+,...,+\omega ^{(n)}x^{(n)}  = \omega^{T} \cdot x$$

由此，定义下面的代码进行梯度上升法的实现：

```python
# 定义Sigmoid函数
def sigmoid(inX):
    return 1.0/(1 + np.exp(- inX))

# 随机的梯度上升法
def gradAscent(dataMatIn, classLabels, numIter = 150):
    # 获得行数和列数，即样本数和特征数
    m, n = np.shape(dataMatIn)
    # 权重初始化
    weights = np.ones(n)
    for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0 + j + i) + 0.01
            randIndex = int(np.random.uniform(0, len(dataIndex)))
            h = sigmoid(sum(dataMatIn[randIndex] * weights))
            error = classLabels[randIndex] - h
            weights = weights + np.dot(alpha * error, dataMatIn[randIndex])
    return weights

weights = gradAscent(dataMat, labelMat)
print(weights)

'''
[-1.91739131 -2.37320272  3.30085298  1.32020706 -2.30328752  0.58413608
      0.84630395 -0.63702599]
'''
```

* 分类器设计

由上，求出了相应的输入特征对应权重，利用对数几率公式，可以简单实现分类的效果，相关设计代码如下：

```python
def classfy(testdir, weights):
    dataMat, labelMat = loadDataSet(testdir)
    dataMat = np.mat(dataMat)
    weights = np.mat(weights)
    h = sigmoid(dataMat * weights.transpose())
    h = h.tolist()
    m = len(h)
    error = 0.0
    for i in range(m):
        if h[i][0] > 0.5:
            print(int(labelMat[i]),'is classfied as: 1')
            if int(labelMat[i])!=1:
                error += 1
                print('error')
        else:
            print(int(labelMat[i]),'is classfied as: 0')
            if int(labelMat[i])!=0:
                error += 1
                print('error')
    print('error rate is:','%.4f' %(error/m))

print(classfy(filename, weights))
'''
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
1 is classfied as: 1
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 0
0 is classfied as: 1
error
0 is classfied as: 0
0 is classfied as: 1
error
0 is classfied as: 0
0 is classfied as: 0
error rate is: 0.1176
'''
```

将训练的数据样本进行测试，可以看出上述分类中，只有两个样本被分类错误了，准确度达到了$88.24%$，分类效果不错。

* `Scikit-Learn`库简单实现`Logistic`分类

简单通过`Scikit-Learn`库实现`Logistic`的分类：

```python
from sklearn.linear_model import LogisticRegression

X, Y = loadDataSet(filename)

clf = LogisticRegression()
clf.fit(X, Y)
y_pred = clf.predict(X)
accuracy = np.mean(Y == y_pred)
print('准确度为：', accuracy)
'''
准确度为： 0.882352941176
'''
```

可以看出，上述实现的准确度与设计的分类器的准确度基本一样，效果不错。

* `Logistic`回归分类的优缺点：

1. 优点：计算代价低，易于理解和实现；
2. 缺点：易欠拟合，分类精度普遍不高。

### 8.2.2 决策树

决策树(`decision tree`)又叫判定树，是基于树结构对样本属性进行分类的分类算法。以二分类为例，当对某个问题进行决策时，会进行一系列的判断或者“子决策”，将每个判定问题可以简单理解为“开”或“关”，当达到一定条件的时候，就进行“开”或“关”的操作，操作就是对决策树的这个问题进行的测试。所有的“子决策”都完成的时候，就构建出了一颗完整的决策树。

一颗典型的决策树包括根结点、若干个内部结点和若干个叶结点；叶结点对应着决策结果，其它每个结点对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。

决策树学习的目的在于产生一颗泛化能力强，即处理未见示例能力强的决策树，基本流程遵循简单直观的“分而治之”(divide-and-conquer)策略。

一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高。

* 信息熵(`information entropy`)

信息熵是度量样本集合纯度最常用的一种指标。在样本集合$D$中，第*k*类样本所占比例为$p_k(k=1,2,...,|y|)$，则$D$的信息熵为：

$$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$

$Ent(D)$的值越小，$D$的纯度越高。

* 信息增益(`information gain`)

离散属性$a$中有$V$个可能取值$\{ a^1,a^2,...,a^V\}$，使用$a$对样本集$D$进行划分，则产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记作$D^v$，$D^v$信息熵即为$Ent(D^v)$。

考虑到不同分支结点包含样本数不同，则给分支结点赋权$\frac{|D^v|}{|D|}$，最后可计算出属性$a$对样本集$D$进行划分所得到的信息增益：

$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

著名的$ID3$算法就是基于信息增益构建的。

一般信息增益越大，则意味使用属性$a$进行划分所获得的“纯度提升”越大。

* 信息增益率(`gain ratio`)

信息增益准则对可数数目较多属性有所偏好。这里考虑一个特殊情况，若分支数目就为样本数，则信息增益就为样本集信息熵，此时信息增益亦越大，此时决策树并不具有泛化能力，无法对新样本进行有效预测。

信息增益率就是为了解决上述问题而产生的。信息增益率的计算公式为：

$$Gain_{ratio}(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

称为属性$a$的“固有值”(intrinsic value)，属性$a$的可能取值数目越多（即$V$越大），则$IV(a)$通常会越大。

基于$ID3$算法改进的$C4.5$算法就是基于信息增益率构建的。

需要注意的是，信息增益率对于可取值数目较少的属性有所偏好。

* 基尼指数(`gini index`)

基尼指数是常用于$CART$决策树的一种划分准则，样本集$D$的“纯度”也可通过这个指数进行确定。基尼指数：

$$Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{'}\neq{k}}p_kp_{k^{'}}=1-\sum_{k=1}^{|y|}p_k^2$$

它反映了从样本集$D$中随机抽取两个样本，其类别标记不一致的概率。

因此$Gini(D)$越小，样本集$D$纯度越高。

定义属性$a$的基尼指数为：

$$Gini_{index(D)}=\sum_{v=1}^{|V|}\frac{|D^v|}{|D|}Gini(D^v)$$

在候选属性集合$A$中，选择使得划分后基尼指数最小的属性做为最优划分属性。

#### 算法实现

采用以下样本集实现决策树

|  编号  |  色泽  |  根蒂  |  敲声  |  纹理  |  脐部  |  触感  |  好瓜  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  1   |  青绿  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  |  是   |
|  2   |  乌黑  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  |  是   |
|  3   |  乌黑  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  |  是   |
|  4   |  青绿  |  蜷缩  |  沉闷  |  清晰  |  凹陷  |  硬滑  |  是   |
|  5   |  浅白  |  蜷缩  |  浊响  |  清晰  |  凹陷  |  硬滑  |  是   |
|  6   |  青绿  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  |  是   |
|  7   |  乌黑  |  稍蜷  |  浊响  |  稍糊  |  稍凹  |  软粘  |  是   |
|  8   |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  硬滑  |  是   |
|  9   |  乌黑  |  稍蜷  |  沉闷  |  稍糊  |  稍凹  |  硬滑  |  否   |
|  10  |  青绿  |  硬挺  |  清脆  |  清晰  |  平坦  |  软粘  |  否   |
|  11  |  浅白  |  硬挺  |  清脆  |  模糊  |  平坦  |  硬滑  |  否   |
|  12  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  软粘  |  否   |
|  13  |  青绿  |  稍蜷  |  浊响  |  稍糊  |  凹陷  |  硬滑  |  否   |
|  14  |  浅白  |  稍蜷  |  沉闷  |  稍糊  |  凹陷  |  硬滑  |  否   |
|  15  |  乌黑  |  稍蜷  |  浊响  |  清晰  |  稍凹  |  软粘  |  否   |
|  16  |  浅白  |  蜷缩  |  浊响  |  模糊  |  平坦  |  硬滑  |  否   |
|  17  |  青绿  |  蜷缩  |  沉闷  |  稍糊  |  稍凹  |  硬滑  |  否   |

```python
import pandas as pd#导入pandas库
import numpy as np#导入numpy库

data = pd.read_csv('822_01.txt', ',', index_col='编号')
labels = list(data.columns)
dataSet = np.array(data).tolist()#处理读入数据为list类型，方便后续计算
```

* 实现根结点信息熵的计算

```python
from math import log

def calcShannonEnt(dataSet):
    numEntries = len(dataSet)#计算样本集的总样本数量
    labelCounts = {}#设置一个空的dict类型变量
    for featVec in dataSet:#遍历每行样本集
        currentLabel = featVec[-1]#选取样本集最后一列，设置为labelCounts变量的key值
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0#key对应value初始化
        labelCounts[currentLabel] += 1#累计value，即计算同类别样本数
    shannonEnt = 0.0#初始化信息熵
    for key in labelCounts:
        prob = float(labelCounts[key]) / numEntries#计算频率
        shannonEnt -= prob * log(prob, 2)#计算信息熵
    return shannonEnt

print(calcShannonEnt(dataSet))
'''
0.9975025463691153
'''
```

可以得到是否“好瓜”，即根结点的信息熵为$0.9975025463691153$。

* 计算不同子属性信息熵

判断是否“好瓜”的有｛色泽，根蒂，敲声，纹理，脐部，触感｝等$6$个子属性，同时以“色泽”属性为例，它有3个可能取值｛青绿，乌黑，浅白｝，分别记为$D^1$(色泽=青绿)、$D^2$(色泽=乌黑)和$D^1$(色泽=浅白)。

如果希望通过上面计算根结点的代码计算不同子属性信息熵，这里就需要对样本集进行划分，选择相应的子属性。这里通过下面的代码实现样本集的划分。

```python
def splitDataSet(dataSet, axis, value):
    #dataSet为样本集
    #axis为子属性下标，如0代表子属性“色泽”
    #value为上述子属性取值
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis + 1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet

newdataSet1=splitDataSet(dataSet, 0, '青绿')#将为“青绿”的样本集合划分出来
'''
[['蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'], ['稍蜷', '浊响', '清晰', '稍凹', '软粘', '是'], ['硬挺', '清脆', '清晰', '平坦', '软粘', '否'], ['稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '否'], ['蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '否']]
'''
newdataSet2=splitDataSet(dataSet, 0, '乌黑')#将为“青绿”的样本集合划分出来
'''
[['蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '是'], ['蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['稍蜷', '浊响', '稍糊', '稍凹', '软粘', '是'], ['稍蜷', '浊响', '清晰', '稍凹', '硬滑', '是'], ['稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '否'], ['稍蜷', '浊响', '清晰', '稍凹', '软粘', '否']]
'''
newdataSet3=splitDataSet(dataSet, 0, '浅白')#将为“青绿”的样本集合划分出来
'''
[['蜷缩', '浊响', '清晰', '凹陷', '硬滑', '是'], ['硬挺', '清脆', '模糊', '平坦', '硬滑', '否'], ['蜷缩', '浊响', '模糊', '平坦', '软粘', '否'], ['稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '否'], ['蜷缩', '浊响', '模糊', '平坦', '硬滑', '否']]
'''

print(calcShannonEnt(newdataSet1))
print(calcShannonEnt(newdataSet2))
print(calcShannonEnt(newdataSet3))
'''
1.0
0.9182958340544896
0.7219280948873623
'''
```

可以得到$D^1$(色泽=青绿)的信息熵为：$1.0$，$D^2$(色泽=乌黑)的信息熵：为$0.9182958340544896$，$D^1$(色泽=浅白)的信息熵为：$0.7219280948873623$。

* 实现信息增益的计算

```python
numFeatures = len(dataSet[0]) - 1#计算子属性的数量
baseEntropy = calcShannonEnt(dataSet)#计算根结点信息熵
columns=['色泽','根蒂','敲声','纹理','脐部','触感']#子属性
for i in range(numFeatures):
    featList = [example[i] for example in dataSet]
    uniqueVals = set(featList)
    newEntropy = 0.0
    for value in uniqueVals:
        #根据子属性及其取值划分样本子集
        subDataSet = splitDataSet(dataSet, i, value)
        prob = len(subDataSet) / float(len(dataSet))#权值
        newEntropy += prob * calcShannonEnt(subDataSet)
        print(value,'的信息熵为：',calcShannonEnt(subDataSet))#不同取值的信息熵
    infoGain = baseEntropy - newEntropy#计算信息增益
    print(columns[i],'信息增益为：',infoGain)
    print('----------------------------------')

'''
青绿 的信息熵为： 1.0
乌黑 的信息熵为： 0.9182958340544896
浅白 的信息熵为： 0.7219280948873623
色泽 信息增益为： 0.10812516526536531
----------------------------------
蜷缩 的信息熵为： 0.9544340029249649
硬挺 的信息熵为： 0.0
稍蜷 的信息熵为： 0.9852281360342516
根蒂 信息增益为： 0.14267495956679288
----------------------------------
清脆 的信息熵为： 0.0
浊响 的信息熵为： 0.9709505944546686
沉闷 的信息熵为： 0.9709505944546686
敲声 信息增益为： 0.14078143361499584
----------------------------------
模糊 的信息熵为： 0.0
清晰 的信息熵为： 0.7642045065086203
稍糊 的信息熵为： 0.7219280948873623
纹理 信息增益为： 0.3805918973682686
----------------------------------
稍凹 的信息熵为： 1.0
平坦 的信息熵为： 0.0
凹陷 的信息熵为： 0.863120568566631
脐部 信息增益为： 0.28915878284167895
----------------------------------
硬滑 的信息熵为： 1.0
软粘 的信息熵为： 0.9709505944546686
触感 信息增益为： 0.006046489176565584
----------------------------------
'''
```

* 基于信息增益选择最优划分属性

一般某个子属性的信息增益越大，意味着使用该属性进行划分的“纯度提升”越大，下面通过改造上述代码，选取最优的划分属性。

```python
# 基于信息增益选择最优划分属性
def chooseBestFeatureToSplit_Gain(dataSet):
    numFeatures = len(dataSet[0]) - 1
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0#初始最优信息增益
    bestFeature = -1#初始最优子属性
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet) / float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)
        infoGain = baseEntropy - newEntropy
        if (infoGain > bestInfoGain):#选择最优子属性
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature

chooseBestFeatureToSplit_Gain(dataSet)
'''
3
'''

上述结果表示在这个样本集中，最优的划分子属性是“纹理”，接下来根结点就通过该子属性进行划分。在该划分之后，通过剩余的其它子属性再进行信息增益的计算得到下个最优划分子属性，迭代进行，直到全部子属性变量完成。

* 基于信息增益率最优划分属性选取

计算信息增益率代码与上文计算信息增益的代码类似，同时下文基于该原则选择最优子属性的代码中有体现，这里不过多赘述。

同样的，基于信息增益率原则也是选择信息增益率最大的为最优划分结点，下面基于信息增益率划分最优子属性。

由于本文采用的样本集是离散型特征的样本，这里的信息增益率和基尼指数选择划分属性的实现并未针对连续型特征进行。后续会加以改进。

```python
# 基于信息增益率选择最优划分属性
def chooseBestFeatureToSplit_GainRatio(dataSet):
    numFeatures = len(dataSet[0]) - 1
    baseEntropy = calcShannonEnt(dataSet)
    bestGainRatio = 0.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        iv = 0.0#初始化“固有值”
        GainRatio = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet) / float(len(dataSet))
            iv -= prob * log(prob, 2)#计算每个子属性“固有值”
            newEntropy += prob * calcShannonEnt(subDataSet)
        infoGain = baseEntropy - newEntropy
        GainRatio = infoGain / iv#计算信息增益率
        if (GainRatio > bestGainRatio):#选择最优节点
            bestGainRatio = GainRatio
            bestFeature = i
    return bestFeature

chooseBestFeatureToSplit_GainRatio(dataSet)
'''
3
'''
```

上述结果表示在这个样本集中，最优的划分子属性同样是“纹理”，接下来的划分实现过程就由该结点开始。

* 计算基尼指数及最优划分属性选取

```python
# 计算基尼指数
def calcGini(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
        Gini = 1.0
        for key in labelCounts:
            prob = float(labelCounts[key]) / numEntries
            Gini -= prob * prob
    return Gini

calcGini(dataSet)#根结点基尼指数
'''
0.49826989619377154
'''
```

* 基于基尼指数选择最优划分属性

```python
# 基于基尼指数选择最优划分属性(只能对离散型特征进行处理)
def chooseBestFeatureToSplit_Gini(dataSet):
    numFeatures = len(dataSet[0]) - 1
    bestGini = 100000.0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newGiniIndex = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet) / float(len(dataSet))
            newGiniIndex += prob * calcGini(subDataSet)
        if (newGiniIndex < bestGini):
            bestGini = newGiniIndex
            bestFeature = i
    return bestFeature

chooseBestFeatureToSplit_Gini(dataSet)
'''
3
'''
```

上述结果同样表示在这个样本集中，最优的划分子属性同样是“纹理”，接下来的划分实现过程也由该结点开始。

* 创建树

决策树的创建过程是迭代进行的，下面的代码是根据子属性的取值数目大小来进行每层根结点的选择的实现过程。即简单理解为选择“下一个根结点”。

```python
import operator

# 选择下一个根结点
def majorityCnt(classList):
    classCount = {}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0#初始化子属性取值的计数
        classCount[vote] += 1#累计
    #根据第二个域，即dict的value降序排序
    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse = True)
    return sortedClassCount[0][0]#返回子属性取值

# 创建树
def createTree(dataSet, labels, chooseBestFeatureToSplit):
    classList = [example[-1] for example in dataSet]#初始化根结点
    if classList.count(classList[0]) == len(classList):#只存在一种取值情况
        return classList[0]
    if len(dataSet[0]) == 1:#样本集只存在一个样本情况
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)#最优划分属性选取
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel: {}}#初始化树
    del (labels[bestFeat])#删除已划分属性
    featValues = [example[bestFeat] for example in dataSet]#初始化下层根结点
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]
        #遍历实现树的创建
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels, chooseBestFeatureToSplit)
    return myTree

chooseBestFeatureToSplit=chooseBestFeatureToSplit_Gain#根据信息增益创建树
# chooseBestFeatureToSplit=chooseBestFeatureToSplit_GainRatio#根据信息增益率创建树
# chooseBestFeatureToSplit=chooseBestFeatureToSplit_Gini#根据基尼指数创建树
myTree = createTree(dataSet, labels, chooseBestFeatureToSplit)

print(myTree)
'''
{'纹理': {'模糊': '否',
      '清晰': {'根蒂': {'硬挺': '否',
        '稍蜷': {'色泽': {'乌黑': {'触感': {'硬滑': '是', '软粘': '否'}}, '青绿': '是'}},
        '蜷缩': '是'}},
      '稍糊': {'触感': {'硬滑': '否', '软粘': '是'}}}}
'''
```

* 分类器实现

需要将划分好的决策树加载出来进行分类，这里定义一个分类的代码如下。

```python
# 分类测试器
def classify(inputTree, featLabels, testVec):
    firstStr = list(inputTree.keys())[0]
    secondDict = inputTree[firstStr]#下一层树
    featIndex = featLabels.index(firstStr)#将Labels标签转换为索引
    for key in secondDict.keys():
        if testVec[featIndex] == key:#判断是否为与分支节点相同，即向下探索子树
            if type(secondDict[key]).__name__ == 'dict':
                #递归实现
                classLabel = classify(secondDict[key], featLabels, testVec)
            else:
                classLabel = secondDict[key]
    return classLabel#返回判断结果

print(classify(myTree, ['色泽','根蒂','敲声','纹理','脐部','触感'],['乌黑','稍蜷','沉闷','稍糊','稍凹','硬滑']))
'''
'否' # 得到分类结果为'否'
'''
```

* 保存和加载树

由于决策树的训练是一件耗时的工作，当碰到较大的样本集的时候，为了下次使用的方便，这里将决策树保存起来。

通过`pickle`模块存储和加载决策树。

```python
# 保存树
def storeTree(inputTree,filename):
    import pickle
    fw = open(filename,'wb+')
    pickle.dump(inputTree,fw)
    fw.close()

# 加载树
def grabTree(filename):
    import pickle
    fr = open(filename,'rb')
    return pickle.load(fr)

storeTree(myTree,'myTree.txt') # 保存到mytree.txt文件中
grabTree('myTree.txt') # 加载保存的决策树
```

* 使用`Scikit-Learn`库实现决策树

基于信息熵简单实现决策树。

```python
import numpy as np
import pandas as pd
from sklearn import tree, preprocessing

'''由于在此库中需要使用数值进行运算，这里需要对样本集进行处理'''
data = pd.read_csv('822_01.txt', ',', index_col='编号')
for col in data.columns:
    data[col] = preprocessing.LabelEncoder().fit_transform(data[col])
labels = np.array(data['好瓜'])
dataSet = np.array(data[data.columns[:-1]])

clf = tree.DecisionTreeClassifier(criterion = 'entropy')#参数criterion = 'entropy'为基于信息熵，‘gini’为基于基尼指数
clf.fit(dataSet, labels)#训练模型
print(clf)

with open("822_01.dot", 'w') as f:#将构建好的决策树保存到tree.dot文件中
    f = tree.export_graphviz(clf,feature_names = np.array(data.columns[:-1]), out_file = f)
```

可以通过`Graphviz`将保存好的`822_01.dot`绘制出来，这里不赘述。

### 8.2.3 朴素贝叶斯法

### 8.2.4 `SVM`