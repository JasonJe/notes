## 8.2 监督学习

>date: 2019-06-20

![](../assets/images/82.jpg)

### 8.2.1 `Logistic`回归

将已知存在的数据点利用一条直线进行拟合（该直线被称为最佳拟合直线）的过程称为回归。

`Logistic`回归分类的主要思想就是依据现有已分类的数据样本建立回归公式，以此进行分类。

回归源于最佳拟合，而`Logistic`回归中训练分类器的做法就是利用一些最优算法寻找符合最佳拟合的拟合参数。

#### `Logistic`分布

设$X$是连续的随机变量，且具有如下的分布函数和密度函数：

$$ F(X) = P(X\leq{x}) = \frac{1}{1+\exp^{-\frac{(x-\mu)}{\gamma}}}$$

$$ f(X) = F'(X) = \frac{\exp^{-\frac{x-\mu}{\gamma}}}{\gamma(1+\exp^{-\frac{x-\mu}{\gamma}})^2}$$

其中$\mu$为位置参数，$\gamma > 0$为形状参数。

下图是$F(X),f(X)$的大致图像，人们又把$F(X)$函数称为$sigmoid$函数（以后会经常见到它）。

![Logistic分布](../assets/images/821_01.jpg)

#### 二项式`Logistic`回归

从`Logistic`分布定义中引申出`Logistic`回归模型。

在上述$X$中，将其分为0、1两类。即随机变量$X$的取值为实数，其分类变量定义为$Y$，每个$x$所属类别可以通过条件概率分布$P(Y|X)$进行表示。
条件概率分布如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x+b)}{1+\exp(\omega \cdot x+b)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x+b)}$$

$x\in R^n$是输入，$Y\in {0, 1}$是输出，$\omega \in R^n$为权值向量，$b \in R$为偏置，$\omega \cdot x$为$\omega$和$x$的内积。

这里将$\omega$和$x$进行扩充，即$\omega = (\omega ^{(1)},\omega ^{(2)},...,\omega ^{(n)},b)$和$x = (x^{(1)},x^{(2)},...,x^{(n)},1)$。

此时的`Logistic`回归模型如下：

$$P(Y=1\mid x) = \frac{\exp(\omega \cdot x)}{1+\exp(\omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\omega \cdot x)}$$

在考虑一个事件发生为$p$，则不发生的概率为$1-p$，那么这个事件发生的几率（odds）为$\frac{p}{1-p}$。对数几率（log odds）为

$$logit(p) = \log{\frac{p}{1-p}}$$

将上述`Logistic`回归模型代入其中可以得到：

$$\log{\frac{P(Y = 1\mid x)}{1-P(Y = 1\mid x)}} = \omega \cdot x$$

可以看出输出$Y = 1$的对数几率是输入$x$的线性函数，即$Y$可以用$x$的线性函数表示。自然地，在`Logistic`回归模型中，$x$的线性函数值$\omega \cdot x$越接近于$+∞$，其概率越接近于1，反之，$\omega \cdot x$越接近于$-∞$，其概率越接近于0。

#### 参数估计

假设给定训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i \in R^n$，$y_i \in \{0,1\}$，利用极大似然估计法估计模型参数。

设$P(Y=1\mid x) =\pi (x)$，则$P(Y=0\mid x) =1-\pi (x)$

似然函数为：

$$\prod_{i=1}^{N} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

对数似然函数为：

$$L( \omega ) = \sum\_{i=1}^{N}[y\_i \log \pi(x)+(1-y\_i)\log(1-\pi(x\_i))]  = \sum\_{i=1}^{N}[y\_i\log(\frac{\pi(x\_i)}{1-\pi(x\_i)})+\log(1-\pi(x\_i))] = \sum\_{i=1}^{N}[y\_i( \omega \cdot x\_i)-\log(1+\exp( \omega \cdot x\_i))]$$

此时对$L(\omega)$求最大值，即得到$\omega$的估计值。
为此，后续利用梯度下降法或者拟牛顿法进行该值的求解。

假设$\omega$的极大似然估计值为$\hat \omega$，那么`Logistic`模型为：

$$P(Y=1\mid x) = \frac{\exp(\hat \omega \cdot x)}{1+\exp(\hat \omega \cdot x)}$$

$$P(Y=0\mid x) = \frac{1}{1+\exp(\hat \omega \cdot x)}$$

#### 梯度法求解估计值

对上述$L(\omega)$中的$\omega$求导，得到：

$$ \frac{\partial}{\partial \omega}L(\omega) = \sum\_{i=1}^{N} y\_i x\_i - \sum\_{i=1}^{N}\frac{exp(\omega \cdot x\_i)}{1+exp(\omega \cdot x\_i )} x\_i = \sum_{i=1}^{N}(y\_i - \pi(x\_i))x\_i $$

令$\frac{\partial}{\partial \omega}L(\omega) =0$，可以求得其最大的$L(\omega)$对应的$\hat \omega$，但由于无法直接求解，故采用梯度下降法进行求解。

由上求导过程可以知道，$L(\omega)$的梯度为

$$\triangledown\_{\omega}L(\omega) =\sum\_{i=1}^{N}(y\_i - \pi(x\_i)) x\_i$$

故迭代公式（此处为梯度上升算法）为

$$\omega = \omega + \alpha \sum_{i=1}^{N}(y_i - \pi(x_i))x_i$$

其中$\alpha$为步长，当$\mid\mid \triangledown_{\omega}L(\omega) \mid\mid < \varepsilon$，即可求得最大的$\hat \omega$，$\varepsilon$为误差。

#### `Logistic`分类器实现



### 8.2.2 决策树

### 8.2.3 朴素贝叶斯法

### 8.2.4 `SVM`