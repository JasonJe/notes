## 8.3 集成学习

>date: 2019-08-08

![](../assets/images/83.jpg)

### 8.3.1 自助法(`Bootstrap`)

在统计学中，`Boostrap`可以指任何一种有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。

自助法能对采样估计的准确性（标准误差、置信区间和偏差）进行比较好的估计，它基本上能够对任何采样分布的统计量进行估计。

`Bootstrap`有两种形式：非参数`Bootstrap`和参数化的`Bootstrap`，但基本思想都是模拟。

* 非参数化`Bootstrap`

假设估计量为$\theta$，样本大小为$N$，从样本中有放回地再抽样$N$个样本，原来每一个样本被抽中的概率相同，均为$\frac{1}{N}$，得到的新样本称为`Bootstrap`样本。重复$B$次后得到$B$个`Bootstrap`样本集，每个样本集都有对应的估计量$\theta_b$，对于$B$个$\theta_b$，可以计算得到标准误差，置信区间，偏差等。

非参数化`Bootstrap`从原始样本中再抽样，得到的`Bootstrap`样本与原始样本有重合。

* 参数化`Bootstrap`

参数化的`Bootstrap`假设总体的分布已知或总体的分布形式已知，可以由样本估计出分布参数，再从参数化的分布中进行再采样得到`Bootstrap`样本。

集成学习的目的就是为了弥补单个模型某方面不稳定或表现较弱的情况而提出的。其基本方法就是组合这里的多个弱监督模型，最终形成一个更好更全面的模型。即使某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

针对其多个模型组合的特点，其数据集的策略也有所变化，即：

* 数据集大：划分成多个小数据集，学习多个模型进行组合；

* 数据集小：利用`Bootstrap`方法进行抽样，得到多个数据集，分别训练多个模型再进行组合。

### 8.3.2 装袋法(`Bagging`)

![Bagging](../assets/images/832_01.jpg)

`Bagging`对训练数据采用`Boostrap`抽样，其主要思想：

1. 从原始样本集中抽取训练集。样本集大小为$N$，从样本中有放回地再抽样$N$个样本。重复$B$次，得到$B$个训练集，每个训练集相互独立。

2. 每次使用一个训练集得到一个模型，$B$个训练集共得到$B$个模型，模型权重一致。

3. 对分类问题：将上步得到的$B$个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。

#### 随机森林(`Random Forrest`)

随机森林使用了[$CART$决策树](./监督学习.md#基尼指数gini-index)作为弱分类器。在普通的决策树中，会从全部的$n$个特征中选择一个最优的特征作为决策树的左右子树，而随机森林则是随机选择一部分特征，从这一部分特征中选择最优特征来作为决策树的左右子树划分。其中这一部分特征的数目记为$n_{sub}$，$n_{sub} < n$。

这样做的方式有助于增强模型的泛化能力。$n_{sub}$越小，模型越健壮，拟合能力下降，方差减小，偏差变大。一般需要通过交叉验证调参来获取一个合适的$n_{sub}$的值。

随机森林步骤：

假设样本集为$D=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$，弱分类器迭代$T$次。

1. 对样本集进行第$t( t = 1, 2, ..., T)$次有放回的随机采样，共采集$m$次，得到第$t$次，包含$m$个样本的采样集$D_t$；

2. 用$D_t$训练第$t$个决策树模型$G_t(x)$，在训练决策树的节点时，从节点上所有的样本特征中随机选择一部分样本特征，再进一步在这部分特征中选择一个最优特征来进行决策树左右子树的划分。

3. 进行分类预测的情况下，$T$个弱分类器$G_t(x)$投票获取分类结果；回归预测的情况下，$T$个弱分类器$G_t(x)$得到的回归结果进行算术平均的值作为最终结果。

* 算法实现



### 8.3.3 提升法(`Boosting`)

![Boosting](../assets/images/833_01.jpg)

`Boosting`是一种与`Bagging`很类似的技术。`Boosting`的思路则是采用重赋权(`re-weighting`)法迭代地训练基分类器，主要思想：

1. 每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果，即根据错误率不断调整样例的权值，错误率越大则权重越大。。

2. 每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。基分类器之间采用序列式的线性加权方式进行组合。

#### `AdaBoost`

### 8.3.4 `GBDT`

### 8.3.5 `XGBoost`

### 8.3.6 `Stacking`

![Stacking](../assets/images/836_01.jpg)

`Stacking`先从初始样本集训练出初级学习器（个体学习器），然后第$j$个初级学习器对第$i$个初始样本集的预测值作为新样本集的第$i$个样本的第$j$个特征值。生成的新样本集（次级训练集）用于训练次级学习器（用于结合的学习器，也称元学习器`Meta Learner`）。

由于次级训练集是通过初级学习器产生的，若直接使用初级学习器的样本集来产生次级训练集，则过拟合风险会比较大，因此，一般是通过使用交叉验证的方式生成次级训练集。