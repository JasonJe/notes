# 8.3 集成学习

>date: 2019-08-08

![](../assets/images/83.jpg)

### 8.3.1 自助法(`Bootstrap`)

在统计学中，`Boostrap`可以指任何一种有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。

自助法能对采样估计的准确性（标准误差、置信区间和偏差）进行比较好的估计，它基本上能够对任何采样分布的统计量进行估计。

`Bootstrap`有两种形式：非参数`Bootstrap`和参数化的`Bootstrap`，但基本思想都是模拟。

* 非参数化`Bootstrap`

假设估计量为$\theta$，样本大小为$N$，从样本中有放回地再抽样$N$个样本，原来每一个样本被抽中的概率相同，均为$\frac{1}{N}$，得到的新样本称为`Bootstrap`样本。重复$B$次后得到$B$个`Bootstrap`样本集，每个样本集都有对应的估计量$\theta_b$，对于$B$个$\theta_b$，可以计算得到标准误差，置信区间，偏差等。

非参数化`Bootstrap`从原始样本中再抽样，得到的`Bootstrap`样本与原始样本有重合。

* 参数化`Bootstrap`

参数化的`Bootstrap`假设总体的分布已知或总体的分布形式已知，可以由样本估计出分布参数，再从参数化的分布中进行再采样得到`Bootstrap`样本。

集成学习的目的就是为了弥补单个模型某方面不稳定或表现较弱的情况而提出的。其基本方法就是组合这里的多个弱监督模型，最终形成一个更好更全面的模型。即使某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

针对其多个模型组合的特点，其数据集的策略也有所变化，即：

* 数据集大：划分成多个小数据集，学习多个模型进行组合；

* 数据集小：利用`Bootstrap`方法进行抽样，得到多个数据集，分别训练多个模型再进行组合。

### 8.3.2 装袋法(`Bagging`)

![Bagging](../assets/images/832_01.jpg)

`Bagging`对训练数据采用`Boostrap`抽样，其主要思想：

1. 从原始样本集中抽取训练集。样本集大小为$N$，从样本中有放回地再抽样$N$个样本。重复$B$次，得到$B$个训练集，每个训练集相互独立。

2. 每次使用一个训练集得到一个模型，$B$个训练集共得到$B$个模型，模型权重一致。

3. 对分类问题：将上步得到的$B$个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。

#### 随机森林(`Random Forrest`)

随机森林使用了[$CART$决策树](./监督学习.md#基尼指数gini-index)作为弱分类器。在普通的决策树中，会从全部的$n$个特征中选择一个最优的特征作为决策树的左右子树，而随机森林则是随机选择一部分特征，从这一部分特征中选择最优特征来作为决策树的左右子树划分。其中这一部分特征的数目记为$n_{sub}$，$n_{sub} < n$。

这样做的方式有助于增强模型的泛化能力。$n_{sub}$越小，模型越健壮，拟合能力下降，方差减小，偏差变大。一般需要通过交叉验证调参来获取一个合适的$n_{sub}$的值。

随机森林步骤：

假设样本集为$D=\left\{\left(x, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots\left(x_{m}, y_{m}\right)\right\}$，弱分类器迭代$T$次。

1. 对样本集进行第$t( t = 1, 2, ..., T)$次有放回的随机采样，共采集$m$次，得到第$t$次，包含$m$个样本的采样集$D_t$；

2. 用$D_t$训练第$t$个决策树模型$G_t(x)$，在训练决策树的节点时，从节点上所有的样本特征中随机选择一部分样本特征，再进一步在这部分特征中选择一个最优特征来进行决策树左右子树的划分。

3. 进行分类预测的情况下，$T$个弱分类器$G_t(x)$投票获取分类结果；回归预测的情况下，$T$个弱分类器$G_t(x)$得到的回归结果进行算术平均的值作为最终结果。

* 算法实现

```python
import numpy as np
import pandas as pd
from collections import Counter

def _calc_gini(data_set):
    '''
    计算基尼指数
    $gini = 1 - \sum_{k = 1}^{K} p_k^2$
    $k$为类别
    '''
    gini = 1
    labels = Counter(data_set[:, -1].tolist()) # 标签列
    for amount in labels.values():
        prob = amount / data_set.shape[0] # 类别 amount 的概率
        gini -= np.power(prob, 2)
    return gini

def _bootstrap(data_set):
    '''
    自助法进行采样
    '''
    m = data_set.shape[0]
    choosed_feature = np.random.choice(m, m, replace = True) # 可重复采样
    train_data = data_set[choosed_feature, :]
    return train_data

def _split(data_set, feature, value):
    '''
    分离数据集，只针对离散型数据集
    '''
    left = data_set[np.nonzero(data_set[:, feature] == value)[0], :]
    right = data_set[np.nonzero(data_set[:, feature] != value)[0], :]
    return left, right

def _choose_best_feature(data_set, max_features):
    '''
    基于基尼指数选取最优特征
    '''
    best_feature = -1
    best_value = 0
    min_gini = np.inf
    split_gini = 0
    n = data_set.shape[1] - 1

    rand_feature = np.random.choice(n, max_features, replace = False) # 随机选择特征，最多 max_features 个

    for feature in rand_feature:
        values = np.unique(data_set[:, feature]) # 获取该特征的唯一值列表
        for value in values:
            left, right = _split(data_set, feature, value) # 按照该特征进行划分
            split_gini = left.shape[0] / data_set.shape[0] * _calc_gini(left) + right.shape[0] / data_set.shape[0] * _calc_gini(right)
            if split_gini < min_gini:
                min_gini = split_gini
                best_feature = feature
                best_value = value

    return best_feature, best_value

def _create_tree(data_set, max_features):
    if data_set.shape[0] == 0:
        return
    if np.unique(data_set[:, -1]).shape[0] == 1:
        return data_set[0, -1]
    
    best_feature, best_value = _choose_best_feature(data_set, max_features)

    tree = {}
    tree['feature'] = best_feature
    tree['value'] = best_value
    left, right = _split(data_set, best_feature, best_value)
    tree['left'] = _create_tree(left, max_features)
    tree['right'] = _create_tree(right, max_features)
    return tree

def fit(data_set, n_estimators, max_features):
    data_set = np.array(data_set)
    rand_forests = []
    for i in range(n_estimators):
        train_data = _bootstrap(data_set)
        tree = _create_tree(train_data, max_features)
        rand_forests.append(tree)
    
    return rand_forests

def _predict_by_tree(tree, test_data):
    if not isinstance(tree, dict): 
        return tree
    feature = tree['feature']
    value = tree['value']
    if test_data[feature] == value:
        return _predict_by_tree(tree['left'], test_data)
    else: 
        return _predict_by_tree(tree['right'], test_data)

def predict(rand_forests, test_data):
    test_data = np.array(test_data)
    prediction = []
    for data in test_data:
        temp = []
        if isinstance(data, np.ndarray):
            for tree in rand_forests:
                temp.append(_predict_by_tree(tree, data))
            prediction.append(Counter(temp).most_common(1)[0][0])
        else:
            for tree in rand_forests:
                temp.append(_predict_by_tree(tree, test_data))
            prediction.append(Counter(temp).most_common(1)[0][0])
            break
    return prediction

if __name__ == "__main__":
    from sklearn.datasets import make_classification # 生成200个2分类的样本，特征数量为100
    data, lables = make_classification(n_samples = 200, n_features = 100,n_classes = 2)
    data_set = np.concatenate((data, lables.reshape(200, 1)), axis=1)
    np.random.shuffle(data_set) # 随机打乱数据

    train_data_set = data_set[:150, :] # 选取 75% 数据进行训练
    rand_forests = fit(train_data_set, n_estimators = 4, max_features = 20)
    prediction = predict(rand_forests, train_data_set[:, : -1])
    correct = [1 if a == b else 0 for a, b in zip(prediction, train_data_set[:, -1])]
    print('训练集的准确率:%.3f%%' % (correct.count(1) / 150 * 100))
    
    test_data_set = data_set[150:, : -1] # 选取 25% 数据进行测试
    test_labels = data_set[150:, -1]
    
    prediction = predict(rand_forests, test_data_set)
    correct = [1 if a == b else 0 for a, b in zip(prediction, test_labels)]
    print('测试集的准确率:%.3f%%' % (correct.count(1) / 50 * 100))

    from sklearn.ensemble import RandomForestClassifier # 使用 Scikit-Learn 内置的随机森林分类模块进行分类

    rf = RandomForestClassifier(max_features = 20, n_estimators = 4) 
    rf.fit(train_data_set[:, :-1], train_data_set[:, -1])
    print(rf)

    prediction = rf.predict(train_data_set[:, :-1])
    correct = [1 if a == b else 0 for a, b in zip(prediction, train_data_set[:, -1])]
    print('训练集的准确率:%.3f%%' % (correct.count(1) / 150 * 100))

    prediction = rf.predict(test_data_set)
    correct = [1 if a == b else 0 for a, b in zip(prediction, test_labels)]
    print('测试集的准确率:%.3f%%' % (correct.count(1) / 50 * 100))
```

最终结果如下：

```
训练集的准确率:92.667%
测试集的准确率:54.000%
C:\ProgramData\Miniconda3\lib\site-packages\sklearn\ensemble\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath_tests import inner1d
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features=20, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
训练集的准确率:96.000%
测试集的准确率:86.000%
```

自定义的随机森林中，训练集的准确率与`Scikit-Learn`相近，但是测试集的准确率有待提高，即需要进一步提高模型的泛化能力。

* 优点：

1. 具有极高的准确率；

2. 随机性的引入，使得随机森林不容易过拟合；

3. 随机性的引入，使得随机森林有很好的抗噪声能力；

4. 能处理很高维度的数据，并且不用做特征选择；

5. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化；

6. 训练速度快，可以得到变量重要性排序；

7. 容易实现并行化。

* 缺点：

1. 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大；

2. 随机森林模型还有许多不好解释的地方，有点算个黑盒模型。

### 8.3.3 提升法(`Boosting`)

![Boosting](../assets/images/833_01.jpg)

`Boosting`是一种与`Bagging`很类似的技术。`Boosting`的思路则是采用重赋权(`re-weighting`)法迭代地训练基分类器，主要思想：

1. 每一轮的训练数据样本赋予一个权重，并且每一轮样本的权重分布依赖上一轮的分类结果，即根据错误率不断调整样例的权值，错误率越大则权重越大。。

2. 每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。基分类器之间采用序列式的线性加权方式进行组合。

#### `AdaBoost`

`Adaboost`是`Adaptive Boosting`的缩写，意指自适应增强。这是一种迭代算法，核心思想就是针对同一个训练集训练不同的弱分类器，然后这些弱分类器集合起来，构成一个更强的强分类器。

`Adaboost`的自适应指的是其能根据每次弱分类器的结果对数据样本进行调整，具体的就是对弱分类器分类错误的样本进行权值的提高，相反，分类正确进行权值的降低，使用加权后的全体样本进行下一个弱分类器的训练，知道错误率达到约定的大小，或者预先到达迭代次数。

其算法步骤如下：

1. 初始化训练样本权值分布，每个样本赋予相同的权值：

$$D_{1}=\left(w_{11}, w_{12} \cdots w_{1 i} \cdots, w_{1 N}\right), w_{1 i}=\frac{1}{N}, i=1,2, \cdots, N$$

2. 进行$M$轮迭代，使用权值分布为$D_m$的样本集来训练弱分类器$G_m(x)$：

$$G_{m}(x) : \quad \chi \rightarrow\{-1,+1\}$$

3. 计算$G_m(x)$在样本集的分类误差率：

$$e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)$$

这个误差率$e_{m}$就是$G_m(x)$在样本集分类错误的样本的权值之和。

4. 计算$G_m(x)$的系数$a_m$，其表示$G_m(x)$在最终分类器的权重，即重要程度：

$$\alpha_{m}=\frac{1}{2} \ln \frac{1-e_{m}}{e_{m}}$$

当$e_{m} \leq \frac{1}{2}$时，$a_m \geq 0$，并且$a_m$随着$e_{m}$的减小而增大，意味着分类误差率越小的弱分类器的在最终的分类器的作用越大。

5. 更新样本数据的权重分布$D_{m+1}=\left(w_{m+1,1}, w_{m+1,2} \cdots w_{m+1, i} \cdots, w_{m+1, N}\right)$：

$$w_{m+1, i}=\frac{w_{m i}}{Z_{m}} e^{-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)}, i=1,2, \cdots, N$$

其中$Z_{m}=\sum_{i=1}^{N} w_{m i} e^{-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)}​$为规范化因子，能使$D_{m+1}​$变成一个概率分布。

6. 组合各个弱分类器，得到最终的分类器$G(x)​$：

$$G(x)=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)$$

* 算法实现

这里采用处理过的马疝病数据集进行模型训练和测试，进行`40`次迭代，即基于`40`个弱分类器构建强分类器。

```python
import numpy as np

def load_dataset(filepath):
    '''
    加载数据集，并转化为矩阵
    '''
    data_mat, label_mat= [], []
    f = open(filepath)
    for line in f.readlines():
        line_list = line.split(',')
        data_mat.append([float(i) for i in line_list[:-1]])
        label_mat.append(float(line_list[-1]))
    data = np.mat(data_mat)
    labels = np.mat(label_mat).T
    return data, labels

def _weak_classifier(data, feature, thresh, thresh_inequal):
    '''
    弱分类器定义，基于阈值对类别进行划分
    '''
    result = np.ones((data.shape[0], 1))
    if thresh_inequal == 'lt':
        result[data[:, feature] <= thresh] = -1.0
    else:
        result[data[:, feature] > thresh] = -1.0
    return result

def _find_best_classifier(data, labels, D):
    '''
    找到最优的分类器，选择能在两类中使得误差降到最低的特征
    '''
    m, n = data.shape
    num_steps = 10.0
    best_classifier = {}
    best_classification_result = np.mat(np.zeros((m, 1)))
    min_error = np.inf # 初始化最小误差为无穷大

    for i in range(n): # 遍历数据集的所有特征
        min_feature = data[:, i].min() # 每列特征中的最小值
        max_feature = data[:, i].max() # 每列特征中的最大值
        step = (max_feature - min_feature) / num_steps # 步长
        for j in range(-1, int(num_steps) + 1): # 
            for ineuqal in ['lt', 'gt']:
                thresh = (min_feature + float(j) * step) # 计算阈值
                prediction = _weak_classifier(data, i, thresh, ineuqal) # 弱分类器预测的结果
                error = np.mat(np.ones((m, 1)))
                error[prediction == labels] = 0 # 计算误差
                weighted_error = D.T * error # 计算权重

                if weighted_error < min_error: # 更新最小误差
                    min_error = weighted_error
                    best_classification_result = prediction.copy()
                    best_classifier['feature'] = i
                    best_classifier['thresh'] = thresh
                    best_classifier['ineq'] = ineuqal
    return best_classifier, min_error, best_classification_result

def adaboost_train(data, labels, max_iter = 40):
    weak_classifier = []
    m = data.shape[0]
    D = np.mat(np.ones((m ,1)) / m) # 样本权重向量
    alpha_classification_result = np.mat(np.zeros((m, 1)))

    for i in range(max_iter): # 迭代 max_iter 次
        best_classifier, error, classification_result = _find_best_classifier(data, labels, D) # 获取该次迭代的最优弱分类器

        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))
        best_classifier['alpha'] = alpha # 计算 alpha，防止分母为零，使用 max(error, 1e-16)

        weak_classifier.append(best_classifier)

        D = np.multiply(D, np.exp(np.multiply(-1 * alpha * labels, classification_result)))
        D = D / D.sum() # 更新权重矩阵

        alpha_classification_result += alpha * classification_result # 投票法更新预测结果
        e_m = np.multiply(np.sign(alpha_classification_result) != labels, np.ones((m ,1))) # 计算错误个数

        error_rate = e_m.sum() / m # 计算错误率
        if error_rate == 0.0:
            break
    return weak_classifier, alpha_classification_result # 返回每轮迭代的最优弱分类器和对应权重

def predict(data, weak_classifier):
    m = data.shape[0]
    alpha_classification_result = np.mat(np.zeros((m, 1)))
    for classifier in weak_classifier:
        result = _weak_classifier(data, classifier['feature'], classifier['thresh'], classifier['ineq'])
        alpha_classification_result += classifier['alpha'] * result # 投票法更新预测结果
    return np.sign(alpha_classification_result)

if __name__ == "__main__":
    data, labels = load_dataset('833_01.txt')
    weak_classifier, alpha_classification_result = adaboost_train(data, labels)
    prediction = predict(data, weak_classifier)
    error = np.mat(np.ones((data.shape[0], 1)))
    print('训练集的准确率:%.3f%%' % float(100 - error[prediction != labels].sum() / data.shape[0] * 100))

    test_data, test_labels = load_dataset('833_02.txt')
    prediction = predict(test_data, weak_classifier)
    error = np.mat(np.ones((test_data.shape[0], 1)))
    print('测试集的准确率:%.3f%%' % float(100 - error[prediction != test_labels].sum() / test_data.shape[0] * 100))

    from sklearn.ensemble import AdaBoostClassifier # 使用 Scikit-Learn 内置的 AdaBoost 分类模块进行分类

    clf = AdaBoostClassifier(n_estimators = 40, random_state = 0)
    clf.fit(data, labels)
    print(clf)

    print('训练集的准确率:%.3f%%' % (clf.score(data, labels) * 100))
    print('测试集的准确率:%.3f%%' % (clf.score(test_data, test_labels) * 100))
```

最终的结果如下：

```
训练集的准确率:80.268%
测试集的准确率:80.597%
AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,
          learning_rate=1.0, n_estimators=40, random_state=0)
训练集的准确率:83.946%
测试集的准确率:76.119%
```

自定义的`AdaBoost`和`Scikit-Learn`实现的`AdaBoost`训练集准确率和测试集准确率相近，在迭代次数适合的情况下，能得到较好的预测模型。如果迭代次数过大，则容易造成过拟合。

### 8.3.4 `GBDT`

`GBDT`（梯度提升树）是一种迭代的决策树算法。在`GBDT`的迭代中，假设前一轮获取的强分类器的$f_{k - 1}(x)$，则本轮迭代的目标是需要找到一个弱分类器$h_k(x)$，让本轮的损失函数$L(x, f_{k - 1}(x) + h_k(x))$最小。

这里的损失函数$L(x, f_{k - 1}(x) + h_k(x))$，从另一角度解释就是，需要在本轮迭代中找到一个新的模型$f_k(x)$，去拟合上一轮模型$f_{k - 1}(x)$未完全拟合真实样本的残差，即$y - f_k(x)$。此时的损失函数为$L(y, f_k(x)) = \frac{1}{2}(y - f_k(x))^2$。

`GBDT`算法可以看做是$K$颗树组成的加法模型：

$$\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), f_{k} \in F$$

$F$是所有树组成的函数空间。

* 目标函数

上述加法模型的目标函数定义为：

$$O b j(\Theta)=L(\Theta)+\Omega(\Theta) = \sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k=1}^{K} \Omega\left(f_{k}\right)$$

其中$L(\Theta) = \sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)$，代表目标函数的损失函数，其最小化表明模型具有较好的拟合训练样本的能力，也预示着其能较好地拟合真实数据，故优化损失函数来避免欠拟合。

$\Omega(\Theta) = \sum_{k=1}^{K} \Omega\left(f_{k}\right)$代表称为正则项，代表模型的复杂度，正则项的优化则鼓励模型学习到一下较为简单的模型，在预测上更为稳定，也意味着更小的方差，优化正则项来避免过拟合。

最小化目标函数意味着同时最小化损失函数和正则项，尽可能平衡模型的偏差和方差(`Bias Variance Trade-off`)。

* 加法模型

采用前向分布算法(`Forward Stagewise Algorithm`)来优化上述的加法模型。基于`Boosting`的思想，从前往后每一步学习一个基函数机器系数，逐步逼近优化目标函数。

即如下的过程：

$$\hat{y}_{i}^{0}=0 \\ \hat{y}_{i}^{1}=f_{1}\left(x_{i}\right)=\hat{y}_{i}^{0}+f_{1}\left(x_{i}\right) \\ \hat{y}_{i}^{2}=f_{1}\left(x_{i}\right)+f_{2}\left(x_{i}\right)=\hat{y}_{i}^{1}+f_{2}\left(x_{i}\right) \\ \cdots \\ \hat{y}_{i}^{t}=\sum_{k=1}^{t} f_{k}\left(x_{i}\right)=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$$

这时候的目标函数为：

$$\begin{aligned} \operatorname{Obj}^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=i}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant } \end{aligned}​$$

* 泰勒公式

**泰勒公式**：设函数$f(x)$在包含$x_0$的某区间$(a, b)$内有$n + 1$阶导数，则对任一$x \in (a, b)$，有$f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime}\left(x_{0}\right)}{2 !}\left(x-x_{0}\right)^{2}+\cdots+\frac{f^{\prime \prime \prime}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+R_{n}(x)$，其中$R_{n}(x)=\frac{f^{(n+1)}(\xi)}{(n+1) !}\left(x-x_{0}\right)^{n+1}$，$\xi$是位于$x_0$与$x$之间的某个值。多项式$\sum_{n=0}^{N} \frac{f^{(n)}(x_0)}{n !}(x-x_0)^{n}$称为函数在$x_0$处的泰勒展开式，$R_n(x)$是泰勒公式的余项且是$(x - x_0)$的高阶无穷小。

由上，将函数$f(x+\Delta x)$在点$x$处二阶展开，得到：

$$f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$$

目标函数是关于$\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$的函数，若将$\hat{y}_{i}^{t-1}$看做上面展开式的$x$，将$f_{t}\left(x_{i}\right)$看做$\Delta x$，则目标函数转化为：

$$O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+$ constant$

其中$g_i = =\partial_{\hat{y}^{t-1}} l\left(y_{i}, \hat{y}^{t-1}\right)$为损失函数的一阶导，$h_i = \partial_{\hat{y}^{t-1}}^{2} l\left(y_{i}, \hat{y}^{t-1}\right)$为损失函数的二阶导。

由于常量在目标函数最小化过程中不起作用，这里移除常数项，得到：

$$O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)$$

这里就将问题简化为如何定义损失函数，计算其一阶导、二阶导，并且通过加法模型来获取最终学习到的模型了。

* `GBDT`算法

一颗生成好的决策树模型，假设其叶子节点个数为$T$，该决策树是由所有叶子节点对应的值组成的向量$w \in R^{T}$，以及一个把特征向量索引到叶子节点索引的函数$q : R^{d} \rightarrow\{1,2, \cdots, T\}$组成的。因此，决策树可以定义为$f_{t}(x)=w_{q(x)}$。

决策树的复杂度可以由正则项$\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$来定义，即决策树的复杂度由生成的叶子节点数量和叶子节点对应的值向量的$L_2$范数决定。

定义集合$I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$为所有划分到叶子节点$j$的训练样本的集合。目标函数根据叶子节点重新组织为$T$个独立的二次函数的和：

$$\begin{aligned} \operatorname{Obj}^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}$$

定义$G_{j}=\sum_{i \in I_{j}} g_{i}, H_{j}=\sum_{i \in I_{j}} h_{j}$，则上式写为：

$$O b j^{(t)}=\sum_{j=1}^{T}\left[G_{i} w_{j}+\frac{1}{2}\left(H_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T$$

假设数的结构固定，即函数$q(x)$确定，令函数$O b j^{(t)}$的一阶导等于$0$，可求得叶子节点$j$对应的值为：

$$w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}$$

此时目标函数的值为：

$$O b j=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T$$

上述的单颗决策树的学习过程大致如下：

1. 枚举所有可能的树结构$q$；

2. 基于公式$O b j=-\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T$为每个$q$键其分数，分数越小说明对应的树结构越好；

3. 根据`2`中的结果来找到最佳的树结构，使用$w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}$为树的每个叶子节点计算预测值。

但是可能树结构数量是无穷的，所以不可能枚举所有可能的树结构，通常情况下，采用贪心策略来生成决策树的每个节点：

1. 从深度为$0$的树开始，对每个叶节点枚举所有可用的特征；

2. 针对每个特征，吧属于该节点的训练样本根据该特征值升序排序（时间复杂度$O(n \log n)$），通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益；

3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点生长出左右两个新的叶子节点，并为每个新节点关联对应的样本集；

4. 回到`1`，递归执行到满足特定条件停止。

计算`2`中收益的方法如下：

假设当前节点$C$的左右孩子分别为$L$和$R$，则收益为:

$$Gain =O b j_{C}-O b j_{L}-O b j_{R} =\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma$$

其中$\gamma$为惩罚项。

* 算法步骤

1. 算法每次迭代生成一颗新的决策树；
2. 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$；
3. 通过贪心策略生成新的决策树，计算每个叶子节点的预测值$w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}$；
4. 把新生成的决策树$f_t(x)$添加到模型中$\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$。

在`4`中，通常在$f_{t}\left(x_{i}\right)$前增加系数$\epsilon$，即公式变为$\hat{y}_{i}^{t-1}+ \epsilon f_{t}\left(x_{i}\right)$，来防止模型过拟合。

### 8.3.5 `XGBoost`

#### `LightGBM`

### 8.3.6 `Stacking`

![Stacking](../assets/images/836_01.jpg)

`Stacking`先从初始样本集训练出初级学习器（个体学习器），然后第$j$个初级学习器对第$i$个初始样本集的预测值作为新样本集的第$i$个样本的第$j$个特征值。生成的新样本集（次级训练集）用于训练次级学习器（用于结合的学习器，也称元学习器`Meta Learner`）。

由于次级训练集是通过初级学习器产生的，若直接使用初级学习器的样本集来产生次级训练集，则过拟合风险会比较大，因此，一般是通过使用交叉验证的方式生成次级训练集。